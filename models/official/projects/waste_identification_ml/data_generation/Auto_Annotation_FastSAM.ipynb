{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1noxfBxYrdhHIPmWwHQkZ-YykM__MnqMJ","timestamp":1740508170028}],"gpuType":"T4","authorship_tag":"ABX9TyNfiIcYWdzjSNDF95Sy/PJF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Auto-Generating COCO Annotations for Instance Segmentation using FastSAM"],"metadata":{"id":"TUxd2yg_Vz7e"}},{"cell_type":"markdown","source":["Instance segmentation requires high-quality annotations, but manual annotation is time-consuming and expensive. This notebook automates the annotation process by leveraging FastSAM, a lightweight and efficient segmentation model, to generate masks from images.\n","\n","The key steps in this pipeline include:\n","\n","\n","\n","1.   Mask Generation with FastSAM - Detects object masks quickly.\n","2.   Post-processing - Reduces errors, removes false detections, and refines results.\n","3.   COCO JSON Conversion - Converts masks into COCO format for training deep learning models."],"metadata":{"id":"H_Rt24yRWBek"}},{"cell_type":"markdown","source":["## Import required libraries and setup"],"metadata":{"id":"6QwU45_aWqVJ"}},{"cell_type":"code","source":["import os\n","HOME = os.getcwd()\n","\n","%cd {HOME}\n","\n","# Clone the FastSAM repo and install the required libraries.\n","!git clone https://github.com/CASIA-IVA-Lab/FastSAM.git\n","!pip -q install -r FastSAM/requirements.txt\n","!pip -q install git+https://github.com/openai/CLIP.git\n","\n","%cd {HOME}/FastSAM"],"metadata":{"id":"Rgua57f6X_mr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from typing import Union\n","import numpy as np\n","import pandas as pd\n","import torch\n","from scipy import ndimage\n","import cv2\n","import skimage\n","from fastsam import FastSAM, FastSAMPrompt\n","import matplotlib.pyplot as plt\n","\n","\n","DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(f\"DEVICE = {DEVICE}\")"],"metadata":{"id":"ZGCFweSHWtVz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Utils\n","\n","_PROPERTIES = (\n","    \"major_axis_length\",\n","    \"minor_axis_length\",\n",")\n","\n","\n","def masks_to_bool(masks: Union[np.ndarray, torch.Tensor]) -> np.ndarray:\n","  \"\"\"Convert masks to boolean format.\n","\n","  Args:\n","      masks: Input masks, either as a NumPy array or a PyTorch tensor.\n","\n","  Returns:\n","      Boolean masks where values are converted to True/False.\n","  \"\"\"\n","  if type(masks) == np.ndarray:\n","      return masks.astype(bool)\n","  return masks.cpu().numpy().astype(bool)\n","\n","\n","def plot_boolean_masks(masks: np.ndarray, masks_per_row: int = 5):\n","    \"\"\"Plots boolean masks in a grid format with a fixed number of masks per row.\n","\n","    Args:\n","        masks: Boolean masks.\n","        masks_per_row: Number of masks to display per row.\n","    \"\"\"\n","    num_masks = masks.shape[0]  # Total number of masks\n","    num_rows = (num_masks + masks_per_row - 1) // masks_per_row  # Compute required rows\n","\n","    fig, axes = plt.subplots(num_rows, masks_per_row, figsize=(masks_per_row * 3, num_rows * 3))\n","\n","    # Flatten axes array in case of a single row\n","    axes = axes.flatten()\n","\n","    for i in range(len(axes)):\n","        if i < num_masks:\n","            axes[i].imshow(masks[i])  # Display mask\n","            axes[i].axis(\"off\")  # Hide axis labels\n","            axes[i].set_title(f\"Mask {i+1}\")  # Set title\n","        else:\n","            axes[i].axis(\"off\")  # Hide empty subplots\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def extract_properties(masks: np.ndarray) -> pd.DataFrame:\n","    \"\"\"Extracts properties of masks and computes additional ratio features.\n","\n","    Args:\n","        masks: Boolean masks.\n","\n","    Returns:\n","        Extracted properties.\n","    \"\"\"\n","    dataframes = []\n","\n","    for mask in masks:\n","      binary_mask = np.where(mask, 1, 0)\n","      df = pd.DataFrame(\n","        skimage.measure.regionprops_table(binary_mask, properties=_PROPERTIES)\n","      )\n","      dataframes.append(df)\n","\n","    features = pd.concat(dataframes, ignore_index=True)\n","    features[\"axis_ratio\"] = features[\"major_axis_length\"] / features[\"minor_axis_length\"]\n","    return features\n","\n","def _is_contained(mask1: np.ndarray, mask2: np.ndarray):\n","  \"\"\"Check if mask1 is entirely contained within mask2.\n","\n","  Args:\n","    mask1: The first mask.\n","    mask2: The second mask.\n","\n","  Returns:\n","    True if mask1 is entirely contained within mask2, False otherwise.\n","  \"\"\"\n","  return np.array_equal(np.logical_and(mask1, mask2), mask1)\n","\n","\n","def _calculate_iou(mask1: np.ndarray, mask2: np.ndarray) -> float:\n","  \"\"\"Calculate the intersection over union (IoU) between two masks.\n","\n","  Args:\n","    mask1: The first mask.\n","    mask2: The second mask.\n","\n","  Returns:\n","    The intersection over union (IoU) between the two masks.\n","  \"\"\"\n","  intersection = np.logical_and(mask1, mask2).sum()\n","  union = np.logical_or(mask1, mask2).sum()\n","  return intersection / union if union != 0 else 0\n","\n","\n","def filter_masks(masks: np.ndarray, iou_threshold: float = 0.8) -> np.ndarray:\n","  \"\"\"Filter the overlapping masks.\n","\n","  Filter the masks based on the intersection over union (IoU) and keep the\n","  biggest masks if they are overlapping.\n","\n","  Args:\n","    masks: The masks to filter.\n","    iou_threshold: The threshold for the intersection over union (IoU) between\n","      two masks.\n","\n","  Returns:\n","    Unique masks.\n","  \"\"\"\n","  # Calculate the area for each mask\n","  areas = np.array([np.sum(mask) for mask in masks])\n","\n","  # Sort the masks based on area in descending order\n","  sorted_indices = np.argsort(areas)[::-1]\n","  sorted_masks = masks[sorted_indices]\n","\n","  unique_masks = []\n","\n","  for i, mask in enumerate(sorted_masks):\n","    keep = True\n","    for j in range(i):\n","      if _calculate_iou(mask, sorted_masks[j]) > iou_threshold or _is_contained(\n","          mask, sorted_masks[j]\n","      ):\n","        keep = False\n","        break\n","    if keep:\n","      unique_masks.append(mask)\n","\n","  return np.array(unique_masks)\n","\n","\n","def keep_largest_component(masks: np.ndarray) -> np.ndarray:\n","    \"\"\"Keeps only the largest connected component in each binary mask.\n","\n","    Args:\n","        masks: Binary masks.\n","\n","    Returns:\n","        Boolean masks with only the largest component retained.\n","    \"\"\"\n","    largest_component_masks = []\n","\n","    for mask in masks:\n","      mask = mask.astype(np.uint8)*255\n","\n","      # Find connected components\n","      num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n","          mask,\n","          connectivity=8\n","      )\n","\n","      # Find the largest component, excluding the background (label 0)\n","      largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n","\n","      # Create a boolean mask for the largest connected component\n","      largest_component_mask = labels == largest_label\n","      largest_component_mask = ndimage.binary_fill_holes(largest_component_mask)\n","      largest_component_masks.append(largest_component_mask)\n","\n","    return np.array(largest_component_masks)\n","\n","\n","def create_coco_annotation_for_single_image(\n","    binary_masks: np.ndarray,\n","    labels: list[str],\n","    image_name: str,\n","    image_height: int,\n","    image_width: int\n","    ):\n","    \"\"\"Creates a COCO annotation JSON.\n","\n","    Create an annotation file for instance segmentation from binary masks and\n","    corresponding labels for a single image.\n","\n","    Args:\n","      binary_masks: List of binary mask arrays corresponding to objects.\n","      labels: List of labels corresponding to each mask in the image.\n","      image_file: Image name.\n","      image_height: Image height.\n","      image_width: Image width.\n","\n","    Returns:\n","      COCO-style annotation JSON as a Python dictionary.\n","    \"\"\"\n","\n","    # COCO structure template\n","    coco_dataset = {\n","        \"images\": [],\n","        \"annotations\": [],\n","        \"categories\": []\n","    }\n","\n","    # Add categories (assume labels are unique)\n","    label_to_id = {label: idx + 1 for idx, label in enumerate(set(labels))}\n","    for label, category_id in label_to_id.items():\n","        coco_dataset[\"categories\"].append({\n","            \"id\": category_id,\n","            \"name\": label,\n","            \"supercategory\": \"object\"\n","        })\n","\n","    # Get the file name and path\n","    file_name = os.path.basename(image_name)\n","\n","    # extract height and width\n","    height, width = image_height, image_width\n","\n","    img_id = 1  # Since it's a single image, you can set the image ID to 1\n","\n","    # Add image information\n","    coco_dataset[\"images\"].append({\n","        \"id\": img_id,\n","        \"width\": width,\n","        \"height\": height,\n","        \"file_name\": file_name\n","    })\n","\n","    # Process each mask in the image\n","    annotation_id = 1\n","    for mask, label in zip(binary_masks, labels):\n","        category_id = label_to_id[label]\n","\n","        # Find contours for the mask and flatten the contour points\n","        contours, _ = cv2.findContours(\n","            mask.astype(np.uint8),\n","            cv2.RETR_EXTERNAL,\n","            cv2.CHAIN_APPROX_SIMPLE\n","        )\n","        segmentation = []\n","        for contour in contours:\n","            contour = contour.flatten().tolist()  # Flatten the contour and convert it to a list\n","            if len(contour) >= 6:  # A valid polygon needs at least 3 points (6 coordinates)\n","                segmentation.append(contour)\n","\n","        # Calculate area and bounding box\n","        area = int(np.sum(mask.astype(bool)))\n","        bbox = cv2.boundingRect(mask.astype(np.uint8))\n","        x, y, w, h = bbox\n","\n","        # Create annotation entry\n","        coco_dataset[\"annotations\"].append({\n","            \"id\": annotation_id,\n","            \"image_id\": img_id,\n","            \"category_id\": category_id,\n","            \"segmentation\": segmentation,  # Segmentation in polygon format\n","            \"area\": area,\n","            \"bbox\": [x, y, w, h],\n","            \"iscrowd\": 0\n","        })\n","\n","        annotation_id += 1\n","\n","    for i in coco_dataset['annotations']:\n","      i['segmentation'] = [max(i['segmentation'], key=len)]\n","\n","    # Return the COCO JSON object\n","    return coco_dataset"],"metadata":{"id":"BDlLA2KRYXD_","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Install FastSAM weights"],"metadata":{"id":"fKgmXOkjaLwD"}},{"cell_type":"code","source":["!mkdir weights\n","!wget -P weights -q https://huggingface.co/spaces/An-619/FastSAM/resolve/main/weights/FastSAM.pt\n","!ls -lh weights"],"metadata":{"id":"HjynXNbJaNgX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load the model"],"metadata":{"id":"RraxgIKxbEul"}},{"cell_type":"code","source":["FAST_SAM_CHECKPOINT_PATH = \"weights/FastSAM.pt\"\n","fast_sam = FastSAM(FAST_SAM_CHECKPOINT_PATH)"],"metadata":{"id":"Pn7JUcfabGYl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"k3tX3Ykh6fuL"}},{"cell_type":"markdown","source":["Fast SAM parameters:\n","\n","\n","\n","*   `retina_masks=True` determines whether the model uses retina masks for generating segmentation masks.\n","*   `imgsz`=1024 sets the input image size to 1024x1024 pixels for processing by the model.\n","*   `conf`=0.4 sets the minimum confidence threshold for object detection.\n","*   `iou`=0.9 sets the minimum intersection over union threshold for non-maximum suppression to filter out duplicate detections.\n","\n","\n","\n","\n"],"metadata":{"id":"s2VOFQTu7I5m"}},{"cell_type":"code","source":["# Import an image.\n","url = (\n","    \"https://raw.githubusercontent.com/tensorflow/models/master/official/\"\n","    \"projects/waste_identification_ml/pre_processing/config/sample_images/\"\n","    \"sample_image_fastsam.jpeg\"\n",")\n","!curl -O {url}"],"metadata":{"id":"JDVzGCC6Z9IH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["IMAGE_PATH = \"sample_image_fastsam.jpeg\"\n","DEVICE = \"cuda\""],"metadata":{"id":"-swo_ZH96e0N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = fast_sam(\n","    source=IMAGE_PATH,\n","    device=DEVICE,\n","    retina_masks=True,\n","    imgsz=1024,\n","    conf=0.5,\n","    iou=0.1)\n","prompt_process = FastSAMPrompt(IMAGE_PATH, results, device=DEVICE)\n","masks = prompt_process.everything_prompt()\n","\n","if len(masks) == 0:\n","  print(\"No masks detected\")\n","masks = masks_to_bool(masks)\n","print(masks.shape)"],"metadata":{"id":"aS6uJkeQ9nee"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_boolean_masks(masks)"],"metadata":{"id":"20o5ofDy_jOu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Postprocessing masks"],"metadata":{"id":"yaRXiMnJPk5D"}},{"cell_type":"markdown","source":["If you notice that Mask5 and Mask6 are the false positives which needs to be removed. We will use different techniques to get rid of such detections."],"metadata":{"id":"xmbUiEoCAd8Q"}},{"cell_type":"code","source":["image = cv2.imread(IMAGE_PATH)\n","image_height, image_width = image.shape[:2]\n","\n","# Remove masks which are bigger than 30% of an image size and lower than 4000\n","# pixels in area.\n","HIGHER_THRESHOLD = 0.3 * image_height * image_width\n","LOWER_THRESHOLD = 4000\n","masks = np.array([mask for mask in masks if LOWER_THRESHOLD < np.sum(mask) < HIGHER_THRESHOLD])\n","print(masks.shape)"],"metadata":{"id":"1RJNPXo4Ax00"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removes masks whose major to minor axis ratio is bigger than 5.\n","features = extract_properties(masks)\n","\n","RATIO_THRESHOLD = 5\n","masks = np.array([mask for mask,ratio in zip(masks, features[\"axis_ratio\"]) if ratio < RATIO_THRESHOLD])\n","print(masks.shape)"],"metadata":{"id":"tW3cD0jwGdn5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Keep the largest component masks if they are connected.\n","mask = keep_largest_component(masks)\n","print(mask.shape)"],"metadata":{"id":"7vHNTs6gLRUe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove overlapped smaller masks and keep the biggest one using IoU.\n","masks = filter_masks(masks)\n","print(masks.shape)"],"metadata":{"id":"pfN4u98sLPtL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_boolean_masks(masks)"],"metadata":{"id":"OUYrwwIBsTYV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create COCO JSON annotation file"],"metadata":{"id":"4qY5oTDSRMpL"}},{"cell_type":"code","source":["# Get the class name of each corresponding mask.\n","labels = ['non-bottle']*len(masks)\n","labels"],"metadata":{"id":"PB8WBm6LRPc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a COCO JSON format file.\n","coco_json = create_coco_annotation_for_single_image(\n","    masks,\n","    labels,\n","    os.path.basename(IMAGE_PATH),\n","    image_height,\n","    image_width\n",")"],"metadata":{"id":"PGdeaLLqRaYX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["coco_json.keys()"],"metadata":{"id":"CDQZpy5qR4tL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["coco_json['images']"],"metadata":{"id":"miN0_A-mR7BJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["coco_json['annotations'][1].keys()"],"metadata":{"id":"qHWcdW-PR83f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(len(masks)):\n","  print(f\"id:{coco_json['annotations'][i]['id']}\\\n","          image_id:{coco_json['annotations'][i]['image_id']}\\\n","          category_id:{coco_json['annotations'][i]['category_id']}\\\n","          area:{coco_json['annotations'][i]['area']}\\\n","          bbox:{coco_json['annotations'][i]['bbox']}\")"],"metadata":{"id":"au0zisweR_up"},"execution_count":null,"outputs":[]}]}