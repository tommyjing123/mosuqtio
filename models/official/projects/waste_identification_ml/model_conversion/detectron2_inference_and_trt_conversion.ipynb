{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtzVZpUyxS5B"
      },
      "source": [
        "# Waste identification with instance segmentation in Detectron2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTrqWFRdxYyJ"
      },
      "source": [
        "Welcome to the Instance Segmentation Colab! This notebook will take you through the steps of running an \"out-of-the-box\" Mask RCNN Instance Segmentation model on image from Detectron2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Gxj-CPxnFe"
      },
      "source": [
        "To finish this task, a proper path for the model and a single image needs to be provided. The path to the labels on which the models are trained is in the waste_identification_ml directory inside the Tensorflow Model Garden repository. The label files are inferred automatically for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgJKrYHEyAEv"
      },
      "source": [
        "## Clone and install Detectron2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXxxDT_QW2mR"
      },
      "outputs": [],
      "source": [
        "# Clone the Detectron2 repository and install the required packages.\n",
        "# Relax as installing packages might take a while.\n",
        "!git clone -q 'https://github.com/facebookresearch/detectron2'\n",
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCWF8h6TuNqZ"
      },
      "outputs": [],
      "source": [
        "# Install supervision package for the postprocessing of output results\n",
        "# from Detectron2 Mask RCNN model.\n",
        "!pip install -q supervision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1oMgVZj4n9n"
      },
      "source": [
        "## Clone the TF Model Garden repo where the waste identification project is located"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHm3oznC4uBe"
      },
      "outputs": [],
      "source": [
        "!git clone --depth 1 https://github.com/tensorflow/models 2\u003e/dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckQUcFtq1P3w"
      },
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdgIDnYe1dgv"
      },
      "outputs": [],
      "source": [
        "# Third-Party Imports\n",
        "import csv\n",
        "import cv2\n",
        "# Detectron2 Imports\n",
        "import detectron2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data.catalog import Metadata\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.structures import Boxes, Instances\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import supervision as sv\n",
        "import torch\n",
        "\n",
        "# Setup Detectron2 Logger\n",
        "setup_logger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R48IuhsN__vG"
      },
      "outputs": [],
      "source": [
        "# @title Utilities\n",
        "\n",
        "\n",
        "def convert_detections_to_instances(\n",
        "    outputs: dict,\n",
        "    image_size: tuple[int, int] = (1024, 1024),\n",
        "    nms_threshold: float = 0.8,\n",
        "    class_agnostic: bool = True,\n",
        ") -\u003e dict[str, Instances]:\n",
        "  \"\"\"Convert Detectron2 model outputs to an Instances object with Non-Maximum Suppression (NMS) applied.\n",
        "\n",
        "  Args:\n",
        "      outputs: Detectron2 model output containing instance predictions.\n",
        "      image_size: Image dimensions (height, width).\n",
        "      nms_threshold: Non-Maximum Suppression (NMS) threshold.\n",
        "      class_agnostic: Whether NMS should be applied in a class-agnostic manner.\n",
        "\n",
        "  Returns:\n",
        "      Reformatted Detectron2 output as {\"instances\": Instances}.\n",
        "  \"\"\"\n",
        "  # Apply NMS and convert to supervision Detections format\n",
        "  detections = sv.Detections.from_detectron2(outputs).with_nms(\n",
        "      threshold=nms_threshold, class_agnostic=class_agnostic\n",
        "  )\n",
        "\n",
        "  # Convert extracted values to PyTorch tensors\n",
        "  bboxes = torch.tensor(detections.xyxy, dtype=torch.float32)\n",
        "  scores = torch.tensor(detections.confidence, dtype=torch.float32)\n",
        "  classes = torch.tensor(detections.class_id, dtype=torch.int64)\n",
        "\n",
        "  # Create an Instances object\n",
        "  output_instances = Instances(image_size)\n",
        "  output_instances.set(\"pred_boxes\", Boxes(bboxes))\n",
        "  output_instances.set(\"scores\", scores)\n",
        "  output_instances.set(\"pred_classes\", classes)\n",
        "\n",
        "  # Add masks if available\n",
        "  if detections.mask is not None:\n",
        "    masks = torch.tensor(detections.mask, dtype=torch.uint8)\n",
        "    output_instances.set(\"pred_masks\", masks)\n",
        "\n",
        "  return {\"instances\": output_instances}\n",
        "\n",
        "\n",
        "def read_csv(file_path: str) -\u003e list[str]:\n",
        "  \"\"\"Reads a CSV file and returns its contents as a list.\n",
        "\n",
        "  This function reads the given CSV file, skips the header, and assumes\n",
        "  there is only one column in the CSV. It returns the contents as a list of\n",
        "  strings.\n",
        "\n",
        "  Args:\n",
        "      file_path: The path to the CSV file.\n",
        "\n",
        "  Returns:\n",
        "      The contents of the CSV file as a list of strings.\n",
        "  \"\"\"\n",
        "  data_list = []\n",
        "  with open(file_path, \"r\") as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "      data_list.append(row[0])\n",
        "  return data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Q3y2r04e57"
      },
      "source": [
        "## Import and load the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2426m2L41Ny"
      },
      "outputs": [],
      "source": [
        "LABELS_PATH = (\n",
        "    'models/official/projects/waste_identification_ml/pre_processing/'\n",
        "    'config/data/45_labels.csv'\n",
        ")\n",
        "\n",
        "labels = read_csv(LABELS_PATH)\n",
        "\n",
        "my_metadata = Metadata()\n",
        "my_metadata.set(thing_classes=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWQkSqN_3aTP"
      },
      "source": [
        "## Import Detectron2 Mask RCNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P64IPCT3fsr"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget https://storage.googleapis.com/tf_model_garden/vision/\\\n",
        "waste_identification_ml/Detectron2_Jan2025_1024_1024.zip\n",
        "\n",
        "unzip Detectron2_Jan2025_1024_1024.zip \u003e /dev/null 2\u003e\u00261"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPM9Sk9sKgS_"
      },
      "source": [
        "## Load the model and perform inference (Non-TRT)\n",
        "\n",
        "You will need to supply the input and output folders below. Among other options, you can use local files or connect to a google drive where you have images. See examples [here](https://colab.sandbox.google.com/notebooks/io.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SLE4h9kgQkE"
      },
      "outputs": [],
      "source": [
        "# Initialize the Detectron2 configuration object\n",
        "cfg = get_cfg()\n",
        "\n",
        "# Load the model configuration from a YAML file.\n",
        "cfg.merge_from_file(\"config.yaml\")\n",
        "\n",
        "# Set the confidence threshold.\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "\n",
        "# Specify the path to the trained model weights.\n",
        "cfg.MODEL.WEIGHTS = \"model_final.pth\"\n",
        "\n",
        "# Create a predictor object using the configured model.\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2l20agB7IHG"
      },
      "outputs": [],
      "source": [
        "HEIGHT = 1024\n",
        "WIDTH = 1024\n",
        "\n",
        "import collections\n",
        "import os\n",
        "\n",
        "INPUT_FOLDER = \"images\"\n",
        "OUTPUT_FOLDER = \"images_output\"\n",
        "LABEL_COUNTS = collections.defaultdict(int)\n",
        "\n",
        "\n",
        "for filename in os.listdir(INPUT_FOLDER):\n",
        "  if filename.lower().endswith(\n",
        "      (\".jpg\", \".jpeg\", \".png\")\n",
        "  ):  # Adjust extensions if needed\n",
        "    input_path = os.path.join(INPUT_FOLDER, filename)\n",
        "    img = cv2.imread(input_path)\n",
        "\n",
        "    original_height, original_width = img.shape[:2]\n",
        "\n",
        "    resized_image = cv2.resize(\n",
        "        img, (WIDTH, HEIGHT), interpolation=cv2.INTER_AREA\n",
        "    )\n",
        "\n",
        "    outputs = predictor(resized_image)\n",
        "    outputs = convert_detections_to_instances(outputs)\n",
        "\n",
        "    # Extract the predicted instances\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "    # Re-scale bounding boxes back to the original image size\n",
        "    scale_x = original_width / WIDTH\n",
        "    scale_y = original_height / HEIGHT\n",
        "    instances.pred_boxes.scale(scale_x, scale_y)\n",
        "\n",
        "    # Resize masks to match the original image size\n",
        "    if instances.has(\"pred_masks\"):\n",
        "      pred_masks = instances.pred_masks.numpy()  # Convert to NumPy array\n",
        "      resized_masks = []\n",
        "\n",
        "      for mask in pred_masks:\n",
        "        resized_mask = cv2.resize(\n",
        "            mask.astype(\"uint8\"),\n",
        "            (original_width, original_height),\n",
        "            interpolation=cv2.INTER_NEAREST,\n",
        "        )\n",
        "        resized_masks.append(resized_mask)\n",
        "\n",
        "      instances.pred_masks = torch.tensor(resized_masks, dtype=torch.uint8)\n",
        "\n",
        "    # Initialize the visualizer with the original image\n",
        "    visualizer = Visualizer(\n",
        "        img_rgb=img,  # Use the original image\n",
        "        metadata=my_metadata,  # Metadata containing class labels, colors, etc.\n",
        "        scale=1,  # Scale factor for visualization\n",
        "    )\n",
        "\n",
        "    # Draw predictions on the original image\n",
        "    visualized_image = visualizer.draw_instance_predictions(\n",
        "        instances\n",
        "    ).get_image()\n",
        "\n",
        "    output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
        "    cv2.imwrite(output_path, visualized_image)\n",
        "    print(f\"Processed {filename} and saved to {output_path}\")\n",
        "\n",
        "    # Count the labels\n",
        "    for label in instances.pred_classes:\n",
        "      label_name = labels[label.item()]  # Get label name from index\n",
        "      LABEL_COUNTS[label_name] += 1  # Increment the count for this label\n",
        "\n",
        "# Print the label counts\n",
        "for label_name, count in LABEL_COUNTS.items():\n",
        "  print(f\"{label_name}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyqLE59oavQq"
      },
      "source": [
        "## TensortRT Conversion and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X53zNeCAbfCl"
      },
      "outputs": [],
      "source": [
        "!pip install -q onnx onnxruntime common\n",
        "!pip install -q git+https://github.com/NVIDIA/TensorRT#subdirectory=tools/onnx-graphsurgeon\n",
        "!git clone -q https://github.com/NVIDIA/TensorRT.git #-b v8.6.1\n",
        "!pip install -q TensorRT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oDk4Mw0ex3O"
      },
      "source": [
        "# Create TRT Sample image\n",
        "\n",
        "We need a 1344x1344 image to provide for calibration during conversion, see docs [here](https://github.com/NVIDIA/TensorRT/blob/main/samples/python/detectron2/README.md)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N8650bBccJH"
      },
      "outputs": [],
      "source": [
        "original_image = cv2.imread('sample_image.jpg')\n",
        "original_height, original_width = original_image.shape[:2]\n",
        "\n",
        "trt_sample_image = cv2.resize(\n",
        "    original_image, (1344, 1344), interpolation=cv2.INTER_AREA\n",
        ")\n",
        "\n",
        "cv2.imwrite('trt_sample_image.jpg', trt_sample_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le0gRKx8iCnN"
      },
      "source": [
        "# Modify export_model.py\n",
        "\n",
        "According to the docs [here](https://github.com/NVIDIA/TensorRT/blob/main/samples/python/detectron2/README.md#detectron-2-deployment), we need to modify the export_model.py script in detectron2/tools/deploy as follows:\n",
        "\n",
        "```\n",
        "aug = T.ResizeShortestEdge(\n",
        "    [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
        ")\n",
        "```\n",
        "\n",
        "--\u003e\n",
        "\n",
        "\n",
        "```\n",
        "aug = T.ResizeShortestEdge(\n",
        "    [1344, 1344], 1344\n",
        ")\n",
        "```\n",
        "\n",
        "This is to match the ultimate trt pipeline required resolution, and ensure proper calibration. Note that ResizeShortestEdge creates a scaling factor that is later used during image inference pre-processing, where infer.py ultimately creates a model training resolution image that it then scales and adds padding to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlD1ZSRObf2s"
      },
      "outputs": [],
      "source": [
        "!python detectron2/tools/deploy/export_model.py \\\n",
        "    --sample-image trt_sample_image.jpg \\\n",
        "    --config-file config.yaml \\\n",
        "    --export-method tracing \\\n",
        "    --format onnx \\\n",
        "    --output ./ \\\n",
        "    MODEL.WEIGHTS model_final.pth \\\n",
        "    MODEL.DEVICE cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDA06Freb7gg"
      },
      "outputs": [],
      "source": [
        "!python TensorRT/samples/python/detectron2/create_onnx.py \\\n",
        "    --exported_onnx model.onnx \\\n",
        "    --onnx converted.onnx \\\n",
        "    --det2_config config.yaml \\\n",
        "    --det2_weights model_final.pth \\\n",
        "    --sample_image trt_sample_image.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEnrWyhscHDi"
      },
      "outputs": [],
      "source": [
        "!python3 TensorRT/samples/python/detectron2/build_engine.py \\\n",
        "--onnx converted.onnx --engine engine32.trt --precision fp32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J84J4iXvkEws"
      },
      "source": [
        "# Change the labels in the infer.py script to match your labels.csv\n",
        "\n",
        "The imported package /TensorRT/samples/python.detectron2/infer.py has a preset list of labels. These need to modified manually to match your labels.csv / metadata\n",
        "\n",
        "The input for inference below can be a file or a folder, and the expected output is image(s) with visualizations and an accompanying .txt file:\n",
        "\n",
        "[Inference in Python reference](https://github.com/NVIDIA/TensorRT/blob/main/samples/python/detectron2/README.md#inference-in-python)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UipdRZyTDU-K"
      },
      "source": [
        "# Run TRT Inference\n",
        "\n",
        "You can also take the TRT converted model here (engine32.trt) and use the script below to run inference from it.\n",
        "\n",
        "The infer.py script comes from\n",
        "https://github.com/NVIDIA/TensorRT/blob/main/samples/python/detectron2/infer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nedMCmTttSWJ"
      },
      "outputs": [],
      "source": [
        "!rm -rf tensorrt_predictions\n",
        "!mkdir tensorrt_predictions\n",
        "!python TensorRT/samples/python/detectron2/infer.py \\\n",
        "    --engine engine32.trt \\\n",
        "    --input images \\\n",
        "    --det2_config config.yaml \\\n",
        "    --output tensorrt_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd1azz14pSRk"
      },
      "source": [
        "# Summarize the results (optional)\n",
        "\n",
        "This colab features trt conversion and inference as a demo, and is not designed to be productionized. However, you can still take a look at your results below by parsing the outputted text files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHoGG2_tpTVE"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "\n",
        "def count_prediction_classes(folder_path):\n",
        "  \"\"\"Count the predicted classes in TensorRT prediction files.\n",
        "\n",
        "  Args:\n",
        "      folder_path (str): Path to the folder containing .txt prediction files\n",
        "\n",
        "  Returns:\n",
        "      Counter: Counts of each predicted class\n",
        "  \"\"\"\n",
        "  # Counter to store class counts\n",
        "  class_counts = Counter()\n",
        "\n",
        "  # Loop through all txt files in the folder\n",
        "  for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "      file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "      # Read the file\n",
        "      try:\n",
        "        with open(file_path, \"r\") as f:\n",
        "          lines = f.readlines()\n",
        "\n",
        "        # Extract class values (last column) from each line\n",
        "        for line in lines:\n",
        "          if line.strip():  # Skip empty lines\n",
        "            columns = line.strip().split()\n",
        "            if len(columns) \u003e= 6:  # Ensure we have enough columns\n",
        "              class_value = int(\n",
        "                  columns[5]\n",
        "              )  # The class value is the 6th column (index 5)\n",
        "              class_counts[class_value] += 1\n",
        "\n",
        "      except Exception as e:\n",
        "        print(f\"Error processing file {filename}: {e}\")\n",
        "\n",
        "  return class_counts\n",
        "\n",
        "\n",
        "folder_path = \"tensorrt_predictions\"\n",
        "\n",
        "# Count classes\n",
        "class_counts = count_prediction_classes(folder_path)\n",
        "\n",
        "# Print results\n",
        "print(\"Class Counts:\")\n",
        "for class_id, count in sorted(class_counts.items()):\n",
        "  print(f\"Class {class_id}: {count}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
