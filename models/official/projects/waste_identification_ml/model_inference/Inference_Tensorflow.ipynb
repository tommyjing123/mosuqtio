{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtlIRiNXWlQ0"
      },
      "source": [
        "# Waste identification with instance segmentation in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohoMgYgXWsIO"
      },
      "source": [
        "Welcome to the Instance Segmentation Colab! This notebook will take you through the steps of running an \"out-of-the-box\" Mask RCNN Instance Segmentation model on image from TF Model Garden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PKG9z4VYPEs"
      },
      "source": [
        "To finish this task, a proper path for the saved models and a single image needs to be provided. The path to the labels on which the models are trained is in the waste_identification_ml directory inside the Tensorflow Model Garden repository. The label files are inferred automatically for both models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7yl9CqgYWvS"
      },
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELUFMVDDAopS"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "from typing import Any, TypedDict, Callable\n",
        "import cv2\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "logging.disable(logging.WARNING)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_77YK3a_BCg_"
      },
      "source": [
        "To visualize the images with the proper detected boxes and segmentation masks, we will use the TensorFlow Object Detection API. To install it we will clone the repo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhk_NujKO0mb"
      },
      "outputs": [],
      "source": [
        "# Clone the tensorflow models repository.\n",
        "!git clone --depth 1 https://github.com/tensorflow/models 2\u003e/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1dYyG55BtWb"
      },
      "outputs": [],
      "source": [
        "sys.path.append('models/research/')\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import visualization_utils as viz_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GO488S78_2GJ"
      },
      "outputs": [],
      "source": [
        "#@title Utilities\n",
        "class ItemDict(TypedDict):\n",
        "  id: int\n",
        "  name: str\n",
        "  supercategory: str\n",
        "\n",
        "\n",
        "def load_model(model_path: str) -\u003e Callable:\n",
        "    \"\"\"Loads a TensorFlow SavedModel and returns a function for making predictions.\n",
        "\n",
        "    Args:\n",
        "      model_path: Path to the TensorFlow SavedModel.\n",
        "\n",
        "    Returns:\n",
        "      A function that can be used to make predictions.\n",
        "    \"\"\"\n",
        "    try:\n",
        "      print('loading model...')\n",
        "      model = tf.saved_model.load(model_path)\n",
        "      print('model loaded!')\n",
        "      detection_fn = model.signatures['serving_default']\n",
        "      return detection_fn\n",
        "    except (OSError, ValueError, KeyError) as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def perform_detection(model: Callable, image: np.ndarray) -\u003e dict[str, np.ndarray]:\n",
        "    \"\"\"Perform Mask R-CNN object detection on an image using the specified model.\n",
        "\n",
        "    Args:\n",
        "        model: A function that can be used to make predictions.\n",
        "        image: A NumPy array representing the image to be processed.\n",
        "\n",
        "    Returns:\n",
        "        Detection results, where keys are output names and values are NumPy arrays.\n",
        "    \"\"\"\n",
        "    detection_results = model(image)\n",
        "    detection_results = {key: value.numpy() for key, value in detection_results.items()}\n",
        "    return detection_results\n",
        "\n",
        "\n",
        "def _read_csv_to_list(file_path: str) -\u003e list[str]:\n",
        "  \"\"\"Reads a CSV file and returns its contents as a list.\n",
        "\n",
        "  This function reads the given CSV file, skips the header, and assumes\n",
        "  there is only one column in the CSV. It returns the contents as a list of\n",
        "  strings.\n",
        "\n",
        "  Args:\n",
        "      file_path: The path to the CSV file.\n",
        "\n",
        "  Returns:\n",
        "      The contents of the CSV file as a list of strings.\n",
        "  \"\"\"\n",
        "  data_list = []\n",
        "  with open(file_path, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "      data_list.append(row[0])  # Assuming there is only one column in the CSV\n",
        "  return data_list\n",
        "\n",
        "\n",
        "def _categories_dictionary(objects: list[str]) -\u003e dict[int, ItemDict]:\n",
        "  \"\"\"This function takes a list of objects and returns a dictionaries.\n",
        "\n",
        "  A dictionary of objects, where each object is represented by a dictionary\n",
        "  with the following keys:\n",
        "    - id: The ID of the object.\n",
        "    - name: The name of the object.\n",
        "    - supercategory: The supercategory of the object.\n",
        "\n",
        "  Args:\n",
        "    objects: A list of strings, where each string is the name of an\n",
        "      object.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of two dictionaries, as described above.\n",
        "  \"\"\"\n",
        "  category_index = {}\n",
        "  for num, obj_name in enumerate(objects, start=1):\n",
        "    obj_dict = {'id': num, 'name': obj_name, 'supercategory': 'objects'}\n",
        "    category_index[num] = obj_dict\n",
        "  return category_index\n",
        "\n",
        "\n",
        "def load_labels(labels_path: str) -\u003e tuple[list[str], dict[int, ItemDict]]:\n",
        "    \"\"\"\n",
        "    Load label mappings from a CSV file and generate category indices.\n",
        "\n",
        "    Args:\n",
        "        labels_path (str): Path to the CSV file containing label mappings.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[int, dict], Dict[int, dict]]:\n",
        "        - A dictionary mapping category IDs to label details.\n",
        "        - A processed category index dictionary.\n",
        "    \"\"\"\n",
        "    labels = _read_csv_to_list(labels_path)\n",
        "    category_index = _categories_dictionary(labels)\n",
        "    return labels, category_index\n",
        "\n",
        "\n",
        "def preprocess_image(path: str, height: int, width: int) -\u003e tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load an image from a file into a NumPy array, resize it, and expand dimensions for batch processing.\n",
        "\n",
        "    Args:\n",
        "        path: The file path to the image.\n",
        "        height: Desired height of the resized image.\n",
        "        width: Desired width of the resized image.\n",
        "\n",
        "    Returns:\n",
        "        original_image: The original image with shape (original_height, original_width, 3).\n",
        "        resized_image: The resized image with shape (1, height, width, 3), suitable for model input.\n",
        "    \"\"\"\n",
        "    original_image = cv2.imread(path)\n",
        "    if original_image is None:\n",
        "        raise FileNotFoundError(f\"Image not found at path: {path}\")\n",
        "\n",
        "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "    resized_image = cv2.resize(original_image, (width, height), interpolation=cv2.INTER_AREA)\n",
        "    resized_image = np.expand_dims(resized_image, axis=0)\n",
        "\n",
        "    return original_image, resized_image\n",
        "\n",
        "\n",
        "def filter_detection(results: dict[str, np.ndarray], valid_indices: np.ndarray) -\u003e dict[str, np.ndarray]:\n",
        "  \"\"\"Filter the detection results based on the valid indices.\n",
        "\n",
        "  Args:\n",
        "    results: The detection results from the model.\n",
        "    valid_indices: The indices of the valid detections.\n",
        "\n",
        "  Returns:\n",
        "    The filtered detection results.\n",
        "  \"\"\"\n",
        "  if np.array(valid_indices).dtype == bool:\n",
        "    new_num_detections = int(np.sum(valid_indices))\n",
        "  else:\n",
        "    new_num_detections = len(valid_indices)\n",
        "\n",
        "  # Define the keys to filter\n",
        "  keys_to_filter = [\n",
        "      'detection_masks',\n",
        "      'detection_masks_resized',\n",
        "      'detection_masks_reframed',\n",
        "      'detection_classes',\n",
        "      'detection_boxes',\n",
        "      'normalized_boxes',\n",
        "      'detection_scores',\n",
        "      'detection_classes_names',\n",
        "  ]\n",
        "\n",
        "  # Apply filtering to the specified keys\n",
        "  filtered_output = {}\n",
        "\n",
        "  for key in keys_to_filter:\n",
        "    if key in results:\n",
        "      if key == 'detection_masks':\n",
        "        filtered_output[key] = results[key][:, valid_indices, :, :]\n",
        "      elif key in ['detection_masks_resized', 'detection_masks_reframed']:\n",
        "        filtered_output[key] = results[key][valid_indices, :, :]\n",
        "      elif key in ['detection_boxes', 'normalized_boxes']:\n",
        "        filtered_output[key] = results[key][:, valid_indices, :]\n",
        "      elif key in ['detection_classes', 'detection_scores', 'detection_classes_names']:\n",
        "        filtered_output[key] = results[key][:, valid_indices]\n",
        "  filtered_output['num_detections'] = np.array([new_num_detections])\n",
        "\n",
        "  return filtered_output\n",
        "\n",
        "\n",
        "\n",
        "def reframe_masks(results: dict[str, np.ndarray], boxes: str, height: int, width: int) -\u003e np.ndarray:\n",
        "  \"\"\"Reframe the masks to an image size.\n",
        "\n",
        "  Args:\n",
        "    results: The detection results from the model.\n",
        "    boxes: The detection boxes.\n",
        "    height: The height of the original image.\n",
        "    width: The width of the original image.\n",
        "\n",
        "  Returns:\n",
        "    The reframed masks.\n",
        "  \"\"\"\n",
        "  detection_masks = results['detection_masks'][0]\n",
        "  detection_boxes = results[boxes][0]\n",
        "  detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "      detection_masks, detection_boxes, height, width\n",
        "  )\n",
        "  detection_masks_reframed = tf.cast(detection_masks_reframed \u003e 0.5, np.uint8)\n",
        "  detection_masks_reframed = detection_masks_reframed.numpy()\n",
        "  return detection_masks_reframed\n",
        "\n",
        "\n",
        "def _calculate_area(mask: np.ndarray) -\u003e int:\n",
        "  \"\"\"Calculate the area of the mask.\n",
        "\n",
        "  Args:\n",
        "    mask: The mask to calculate the area of.\n",
        "\n",
        "  Returns:\n",
        "    The area of the mask.\n",
        "  \"\"\"\n",
        "  return np.sum(mask)\n",
        "\n",
        "\n",
        "def _calculate_iou(mask1: np.ndarray, mask2: np.ndarray) -\u003e float:\n",
        "  \"\"\"Calculate the intersection over union (IoU) between two masks.\n",
        "\n",
        "  Args:\n",
        "    mask1: The first mask.\n",
        "    mask2: The second mask.\n",
        "\n",
        "  Returns:\n",
        "    The intersection over union (IoU) between the two masks.\n",
        "  \"\"\"\n",
        "  intersection = np.logical_and(mask1, mask2).sum()\n",
        "  union = np.logical_or(mask1, mask2).sum()\n",
        "  return intersection / union if union != 0 else 0\n",
        "\n",
        "\n",
        "def _is_contained(mask1: np.ndarray, mask2: np.ndarray) -\u003e bool:\n",
        "  \"\"\"Check if mask1 is entirely contained within mask2.\n",
        "\n",
        "  Args:\n",
        "    mask1: The first mask.\n",
        "    mask2: The second mask.\n",
        "\n",
        "  Returns:\n",
        "    True if mask1 is entirely contained within mask2, False otherwise.\n",
        "  \"\"\"\n",
        "  return np.array_equal(np.logical_and(mask1, mask2), mask1)\n",
        "\n",
        "\n",
        "def filter_masks(masks: np.ndarray, iou_threshold=0.8, area_threshold=None) -\u003e np.ndarray:\n",
        "  \"\"\"Filter the overlapping masks.\n",
        "\n",
        "  Filter the masks based on the area and intersection over union (IoU).\n",
        "\n",
        "  Args:\n",
        "    masks: The masks to filter.\n",
        "    iou_threshold: The threshold for the intersection over union (IoU) between\n",
        "      two masks.\n",
        "    area_threshold: The threshold for the area of the mask.\n",
        "\n",
        "  Returns:\n",
        "    The indices of the unique masks.\n",
        "  \"\"\"\n",
        "  # Calculate the area for each mask\n",
        "  areas = np.array([_calculate_area(mask) for mask in masks])\n",
        "\n",
        "  # Sort the masks based on area in descending order\n",
        "  sorted_indices = np.argsort(areas)[::-1]\n",
        "  sorted_masks = masks[sorted_indices]\n",
        "  sorted_areas = areas[sorted_indices]\n",
        "\n",
        "  unique_indices = []\n",
        "\n",
        "  for i, mask in enumerate(sorted_masks):\n",
        "    if (area_threshold is not None and sorted_areas[i] \u003e area_threshold) or sorted_areas[i] \u003c 4000:\n",
        "      continue\n",
        "\n",
        "    keep = True\n",
        "    for j in range(i):\n",
        "      if _calculate_iou(mask, sorted_masks[j]) \u003e iou_threshold or _is_contained(\n",
        "          mask, sorted_masks[j]\n",
        "      ):\n",
        "        keep = False\n",
        "        break\n",
        "    if keep:\n",
        "      unique_indices.append(sorted_indices[i])\n",
        "\n",
        "  return unique_indices\n",
        "\n",
        "\n",
        "def adjust_image_size(height: int, width: int, min_size: int) -\u003e tuple[int, int]:\n",
        "  \"\"\"Adjust the image size to ensure both dimensions are at least 1024.\n",
        "\n",
        "  Args:\n",
        "    height: The height of the image.\n",
        "    width: The width of the image.\n",
        "    min_size: Minimum size of the image dimension needed.\n",
        "\n",
        "  Returns:\n",
        "    The adjusted height and width of the image.\n",
        "  \"\"\"\n",
        "  if height \u003c min_size or width \u003c min_size:\n",
        "    return height, width\n",
        "\n",
        "  # Calculate the scale factor to ensure both dimensions remain at least 1024\n",
        "  scale_factor = min(height / min_size, width / min_size)\n",
        "\n",
        "  new_height = int(height / scale_factor)\n",
        "  new_width = int(width / scale_factor)\n",
        "\n",
        "  return new_height, new_width\n",
        "\n",
        "\n",
        "def display_bbox_masks_labels(\n",
        "    result: dict[Any, np.ndarray],\n",
        "    image: np.ndarray,\n",
        "    category_index: dict[int, dict[str, str]],\n",
        "    threshold: float,\n",
        ") -\u003e None:\n",
        "  \"\"\"Saves an image with visualized bounding boxes, labels, and masks.\n",
        "\n",
        "  This function takes the output from Mask R-CNN, copies the original image,\n",
        "  and applies visualizations of detection boxes, classes, and scores.\n",
        "  If available, it also applies segmentation masks. The result is an image that\n",
        "  juxtaposes the original with the annotated version, saved to the specified\n",
        "  folder.\n",
        "\n",
        "  Args:\n",
        "    result: The output from theMask RCNN model, expected to contain detection\n",
        "      boxes, classes, scores, reframed detection masks, etc.\n",
        "    image: The original image as a numpy array.\n",
        "    file_name: The filename for saving the output image.\n",
        "    folder: The folder path where the output image will be saved.\n",
        "    category_index: A dictionary mapping class IDs to class labels.\n",
        "    threshold: Value between 0 and 1 to filter out the prediction results.\n",
        "  \"\"\"\n",
        "  image_new = image.copy()\n",
        "  image_new = cv2.cvtColor(image_new, cv2.COLOR_BGR2RGB)\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_new,\n",
        "      result['normalized_boxes'][0],\n",
        "      (result['detection_classes'][0] + 0).astype(int),\n",
        "      result['detection_scores'][0],\n",
        "      category_index=category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=100,\n",
        "      min_score_thresh=threshold,\n",
        "      agnostic_mode=False,\n",
        "      instance_masks=result.get('detection_masks_reframed', None),\n",
        "      line_thickness=4,\n",
        "  )\n",
        "  return image_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7d00cJH-68Z"
      },
      "outputs": [],
      "source": [
        "# Path to a sample image stored in the repo.\n",
        "IMAGES_FOR_TEST = {\n",
        "    'Image1': (\n",
        "        'models/official/projects/waste_identification_ml/pre_processing/'\n",
        "        'config/sample_images/image_2.png'\n",
        "    )\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XjfDEq--UlE"
      },
      "source": [
        "## Import and load pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ435YHN3Lr-"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget https://storage.googleapis.com/tf_model_garden/vision/\\\n",
        "waste_identification_ml/Jan2025_ver2_merged_1024_1024.zip -q\n",
        "\n",
        "unzip Jan2025_ver2_merged_1024_1024.zip \u003e /dev/null 2\u003e\u00261"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMRVdtYEN5bg"
      },
      "outputs": [],
      "source": [
        "detection_fn = load_model('Jan2025_ver2_merged_1024_1024/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6mmyLsOJicF"
      },
      "source": [
        "## Load label map data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM2A29OrJqaU"
      },
      "source": [
        "Label maps correspond index numbers to category names, so that when our convolution network predicts 5, we know that this corresponds to airplane. Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine.\n",
        "\n",
        "We will load our labels from the same repository that we loaded the TF Object Detection API from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RUzrh0uegqt"
      },
      "outputs": [],
      "source": [
        "LABELS_PATH = (\n",
        "    'models/official/projects/waste_identification_ml/pre_processing/'\n",
        "    'config/data/45_labels.csv'\n",
        ")\n",
        "\n",
        "labels, category_index = load_labels(LABELS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkdD8-QvGZ23"
      },
      "source": [
        "## Loading and pre-process an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSXQF57FGba5"
      },
      "source": [
        "Let's try the model on a simple image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVdlDwchGim_"
      },
      "source": [
        "Note: when using images with an alpha channel, the model expect 3 channels images and the alpha will count as a 4th."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7ZS7gHgGk9f"
      },
      "outputs": [],
      "source": [
        "# The model is trained on 1024 x 1024 image dimensions\n",
        "HEIGHT = 1024\n",
        "WIDTH = 1024\n",
        "IMAGE_PATH = (\n",
        "    'models/official/projects/waste_identification_ml/pre_processing/'\n",
        "    'config/sample_images/image_2.png'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPrNf-NnOnL3"
      },
      "outputs": [],
      "source": [
        "original_image, resized_image = preprocess_image(IMAGE_PATH, HEIGHT, WIDTH)\n",
        "input_tensor = tf.convert_to_tensor(resized_image, dtype=tf.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IhKjOZnQDJh"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(original_image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3r73X-FGzz-"
      },
      "source": [
        "## Perform inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqW1z96LGzmZ"
      },
      "outputs": [],
      "source": [
        "# Running inference with bith the models.\n",
        "result = perform_detection(detection_fn, input_tensor)\n",
        "print(f'Total number of detections: {result[\"num_detections\"][0]}')\n",
        "print(result.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "332GbmRuG5A9"
      },
      "source": [
        "## Process the output to remove overlapping or duplicate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly7Uzms9R9yP"
      },
      "outputs": [],
      "source": [
        "# Prediction threshold.\n",
        "PREDICTION_THRESHOLD = 0.50\n",
        "area_threshold = None\n",
        "\n",
        "if result[\"num_detections\"][0]:\n",
        "  scores = result[\"detection_scores\"][0]\n",
        "  filtered_indices = scores \u003e PREDICTION_THRESHOLD\n",
        "  result = filter_detection(result, filtered_indices)\n",
        "  print(\n",
        "      \"Total number of detections after threshold:\"\n",
        "      f\" {result['num_detections'][0]}\"\n",
        "  )\n",
        "\n",
        "if result[\"num_detections\"][0]:\n",
        "  # Normalize the bounding boxes according to the resized image size.\n",
        "  result[\"normalized_boxes\"] = result[\"detection_boxes\"].copy()\n",
        "  result[\"normalized_boxes\"][:, :, [0, 2]] /= HEIGHT\n",
        "  result[\"normalized_boxes\"][:, :, [1, 3]] /= WIDTH\n",
        "\n",
        "  # Adjust the image size to ensure both dimensions are at least 1024\n",
        "  # for saving images with bbx and masks.\n",
        "  height_plot, width_plot = adjust_image_size(\n",
        "      original_image.shape[0], original_image.shape[1], 1024\n",
        "  )\n",
        "  image_plot = cv2.resize(\n",
        "      original_image,\n",
        "      (width_plot, height_plot),\n",
        "      interpolation=cv2.INTER_AREA,\n",
        "  )\n",
        "  # Reframe the masks to the new size.\n",
        "  result[\"detection_masks_reframed\"] = reframe_masks(\n",
        "      result, \"normalized_boxes\", height_plot, width_plot\n",
        "  )\n",
        "\n",
        "  # Filter the prediction results based on the area threshold and\n",
        "  # remove the overlapping masks.\n",
        "  unique_indices = filter_masks(\n",
        "      result[\"detection_masks_reframed\"],\n",
        "      iou_threshold=0.08,\n",
        "      area_threshold=area_threshold,\n",
        "  )\n",
        "  result = filter_detection(result, unique_indices)\n",
        "  print(\n",
        "      \"Total number of detections after filtering:\"\n",
        "      f\" {result['num_detections'][0]}\"\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkN8UgZ0HcM-"
      },
      "source": [
        "## Visualization of masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5l3TrsiUpS7"
      },
      "outputs": [],
      "source": [
        "labeled_image = display_bbox_masks_labels(\n",
        "    result,\n",
        "    image_plot,\n",
        "    category_index,\n",
        "    PREDICTION_THRESHOLD\n",
        ")\n",
        "%matplotlib inline\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.axis('off')\n",
        "plt.imshow(labeled_image)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
