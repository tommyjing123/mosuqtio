{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtlIRiNXWlQ0"
      },
      "source": [
        "# Waste identification with instance segmentation in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohoMgYgXWsIO"
      },
      "source": [
        "This Colab notebook demonstrates an end-to-end pipeline for object detection, feature extraction, object tracking, and data aggregation using the Mask R-CNN model from Detectron2.\n",
        "Key Steps in the Notebook:\n",
        "\n",
        "\n",
        "\n",
        "*   Object Detection and segmentation – Detect objects in a set of images using Mask R-CNN.\n",
        "*   Feature Extraction \u0026 Tracking – Extract object features and track them across multiple frames to eliminate duplicate counts.\n",
        "*   Color Detection – Identify the color of each detected object.\n",
        "*   Postprocessing – Aggregate tracking results and apply filtering to reduce false positives and false negatives.\n",
        "*   Save detection and tracking results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PKG9z4VYPEs"
      },
      "source": [
        "To finish this task, a proper path for the trained model and images need to be provided. The path to the labels on which the models are trained is in the waste_identification_ml directory inside the Tensorflow Model Garden repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2J5IWCgAOxC"
      },
      "source": [
        "This notebook will output 3 folders and 1 csv file :\n",
        "\n",
        "\n",
        "*   **prediction_folder** : Will contain prediction results with bbox and masks.\n",
        "*   **tracking** : Will contain tracking visualization.\n",
        "*   **cropped_objects** : Will contain category level detected objects.\n",
        "*   **count.csv** : Will contain the individual counts of each category.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98YSgFKZaKVe"
      },
      "source": [
        "## Install Detectron2 and RESTART the runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHZ87DkPaN_A"
      },
      "outputs": [],
      "source": [
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ELUFMVDDAopS"
      },
      "outputs": [],
      "source": [
        "#@title Imports and Setup\n",
        "\n",
        "!pip install -q supervision trackpy openpyxl==3.1.2\n",
        "\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "from typing import Any, TypedDict, Callable\n",
        "import cv2\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import natsort\n",
        "import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "import pandas as pd\n",
        "import skimage\n",
        "import datetime\n",
        "import trackpy as tp\n",
        "import shutil\n",
        "import supervision as sv\n",
        "\n",
        "# Detectron2 Utilities\n",
        "import torch\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.structures import Instances, Boxes\n",
        "from detectron2.data.catalog import Metadata\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "setup_logger()\n",
        "\n",
        "\n",
        "logging.disable(logging.WARNING)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5_XxVSFATYZ"
      },
      "outputs": [],
      "source": [
        "# Connect to Google drive if your data is stored there.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "try:\n",
        "  !ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "  print('Successful')\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  print('Not successful')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAQ8ymV-Cgqp"
      },
      "outputs": [],
      "source": [
        "# # Connect to GCP bucket if your data is store there and copy them locally.\n",
        "# !gcloud init\n",
        "# gsutil -m cp -r gs://input ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_77YK3a_BCg_"
      },
      "source": [
        "To visualize the images with the proper detected boxes and segmentation masks, we will use the TensorFlow Object Detection API. To install it we will clone the repo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhk_NujKO0mb"
      },
      "outputs": [],
      "source": [
        "# Clone the tensorflow models repository.\n",
        "!git clone --depth 1 https://github.com/tensorflow/models 2\u003e/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1dYyG55BtWb"
      },
      "outputs": [],
      "source": [
        "sys.path.append('models/official/projects/waste_identification_ml/model_inference/')\n",
        "import color_and_property_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mvErM8kUirE2"
      },
      "outputs": [],
      "source": [
        "#@title Utilities\n",
        "\n",
        "_PROPERTIES = (\n",
        "    'area',\n",
        "    'bbox',\n",
        "    'convex_area',\n",
        "    'bbox_area',\n",
        "    'major_axis_length',\n",
        "    'minor_axis_length',\n",
        "    'eccentricity',\n",
        "    'centroid',\n",
        "    'label',\n",
        "    'mean_intensity',\n",
        "    'max_intensity',\n",
        "    'min_intensity',\n",
        "    'perimeter'\n",
        ")\n",
        "\n",
        "def convert_detections_to_instances(\n",
        "    outputs: dict,\n",
        "    image_size: tuple[int, int] = (1024, 1024),\n",
        "    nms_threshold: float = 0.8,\n",
        "    class_agnostic: bool = True\n",
        ") -\u003e dict[str, Instances]:\n",
        "    \"\"\"Convert Detectron2 model outputs to an Instances object with Non-Maximum Suppression (NMS) applied.\n",
        "\n",
        "    Args:\n",
        "        outputs: Detectron2 model output containing instance predictions.\n",
        "        image_size: Image dimensions (height, width).\n",
        "        nms_threshold: Non-Maximum Suppression (NMS) threshold.\n",
        "        class_agnostic: Whether NMS should be applied in a class-agnostic manner.\n",
        "\n",
        "    Returns:\n",
        "        Reformatted Detectron2 output as {\"instances\": Instances}.\n",
        "    \"\"\"\n",
        "    # Apply NMS and convert to supervision Detections format\n",
        "    detections = (\n",
        "        sv.Detections.from_detectron2(outputs)\n",
        "        .with_nms(threshold=nms_threshold, class_agnostic=class_agnostic)\n",
        "    )\n",
        "\n",
        "    # Convert extracted values to PyTorch tensors\n",
        "    bboxes = torch.tensor(detections.xyxy, dtype=torch.float32)\n",
        "    scores = torch.tensor(detections.confidence, dtype=torch.float32)\n",
        "    classes = torch.tensor(detections.class_id, dtype=torch.int64)\n",
        "\n",
        "    # Create an Instances object\n",
        "    output_instances = Instances(image_size)\n",
        "    output_instances.set(\"pred_boxes\", Boxes(bboxes))\n",
        "    output_instances.set(\"scores\", scores)\n",
        "    output_instances.set(\"pred_classes\", classes)\n",
        "\n",
        "    # Add masks if available\n",
        "    if detections.mask is not None:\n",
        "        masks = torch.tensor(detections.mask, dtype=torch.uint8)\n",
        "        output_instances.set(\"pred_masks\", masks)\n",
        "\n",
        "    return {\"instances\": output_instances}\n",
        "\n",
        "\n",
        "def read_csv(file_path: str) -\u003e list[str]:\n",
        "  \"\"\"Reads a CSV file and returns its contents as a list.\n",
        "\n",
        "  This function reads the given CSV file, skips the header, and assumes\n",
        "  there is only one column in the CSV. It returns the contents as a list of\n",
        "  strings.\n",
        "\n",
        "  Args:\n",
        "      file_path: The path to the CSV file.\n",
        "\n",
        "  Returns:\n",
        "      The contents of the CSV file as a list of strings.\n",
        "  \"\"\"\n",
        "  data_list = []\n",
        "  with open(file_path, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "      data_list.append(row[0])\n",
        "  return data_list\n",
        "\n",
        "\n",
        "def adjust_image_size(height: int, width: int, min_size: int) -\u003e tuple[int, int]:\n",
        "  \"\"\"Adjust the image size to ensure both dimensions are at least 1024.\n",
        "\n",
        "  Args:\n",
        "    height: The height of the image.\n",
        "    width: The width of the image.\n",
        "    min_size: Minimum size of the image dimension needed.\n",
        "\n",
        "  Returns:\n",
        "    The adjusted height and width of the image.\n",
        "  \"\"\"\n",
        "  if height \u003c min_size or width \u003c min_size:\n",
        "    return height, width\n",
        "\n",
        "  # Calculate the scale factor to ensure both dimensions remain at least 1024\n",
        "  scale_factor = min(height / min_size, width / min_size)\n",
        "\n",
        "  new_height = int(height / scale_factor)\n",
        "  new_width = int(width / scale_factor)\n",
        "\n",
        "  return new_height, new_width\n",
        "\n",
        "\n",
        "def dilated_largest_component(mask: np.ndarray) -\u003e np.ndarray:\n",
        "    \"\"\"Extracts the largest connected component and fills holes.\n",
        "\n",
        "    Args:\n",
        "        mask: Input binary mask (2D numpy array).\n",
        "\n",
        "    Returns:\n",
        "        Binary mask of the largest connected component.\n",
        "    \"\"\"\n",
        "    mask = mask.astype(np.uint8)*255\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
        "    largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
        "    largest_component_mask = np.zeros(mask.shape, dtype=\"uint8\")\n",
        "    largest_component_mask[labels == largest_label] = 1\n",
        "    largest_component_mask = ndimage.binary_fill_holes(largest_component_mask).astype(int)\n",
        "    return largest_component_mask\n",
        "\n",
        "\n",
        "def extract_properties(image, masks):\n",
        "  \"\"\"Extract properties of the mask.\n",
        "\n",
        "  Args:\n",
        "    image: Corresponding image of the mask.\n",
        "    masks: The masks to extract properties from.\n",
        "\n",
        "  Returns:\n",
        "    The extracted properties.\n",
        "  \"\"\"\n",
        "  list_of_df = []\n",
        "  for mask in masks:\n",
        "    mask = np.where(mask, 1, 0)\n",
        "    df = pd.DataFrame(\n",
        "        skimage.measure.regionprops_table(mask, intensity_image=image, properties=_PROPERTIES)\n",
        "    )\n",
        "    list_of_df.append(df)\n",
        "  features = pd.concat(list_of_df, ignore_index=True)\n",
        "  features.rename(\n",
        "      columns={\n",
        "          'centroid-0': 'y',\n",
        "          'centroid-1': 'x',\n",
        "          'bbox-0': 'bbox_0',\n",
        "          'bbox-1': 'bbox_1',\n",
        "          'bbox-2': 'bbox_2',\n",
        "          'bbox-3': 'bbox_3',\n",
        "      },\n",
        "      inplace=True,\n",
        "  )\n",
        "  return features\n",
        "\n",
        "\n",
        "def get_image_creation_time(image_path):\n",
        "  \"\"\"\n",
        "  Retrieves the creation time of an image, trying multiple methods.\n",
        "\n",
        "  Args:\n",
        "    image_path: The path to the image file.\n",
        "\n",
        "  Returns:\n",
        "    A string representing the creation time in the format \"%Y-%m-%d %H:%M:%S\" if\n",
        "    found, otherwise returns \"Creation time not found\".\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # 1. Try EXIF data (if available)\n",
        "    image = Image.open(image_path)\n",
        "    exif_data = image._getexif()\n",
        "    if exif_data:\n",
        "      datetime_tag_id = 36867  # Tag ID for \"DateTimeOriginal\"\n",
        "      datetime_str = exif_data.get(datetime_tag_id)\n",
        "      if datetime_str:\n",
        "        datetime_obj = datetime.datetime.strptime(datetime_str, \"%Y:%m:%d %H:%M:%S\")\n",
        "        return datetime_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # 2. Try file modification time (less accurate, but better than nothing)\n",
        "    file_modified_time = os.path.getmtime(image_path)\n",
        "    datetime_obj = datetime.datetime.fromtimestamp(file_modified_time)\n",
        "    return datetime_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    return \"Image not found\"\n",
        "  except Exception as e:\n",
        "    return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "def process_tracking_result(df):\n",
        "    \"\"\"Process the tracking result dataframe.\n",
        "\n",
        "    Args:\n",
        "      df: Dataframe to be aggregated.\n",
        "\n",
        "    Returns:\n",
        "      Processed dataframe.\n",
        "    \"\"\"\n",
        "    # Get class information with the new include_groups parameter\n",
        "    class_info = df.groupby('particle', as_index=False).apply(\n",
        "        select_class_with_scores,\n",
        "        include_groups=False\n",
        "    )\n",
        "\n",
        "    grouped = df.groupby('particle').agg({\n",
        "        'source_name': 'first',\n",
        "        'image_name': 'first',\n",
        "        'detection_scores': 'max',\n",
        "        'creation_time': 'first',\n",
        "        'bbox_0': 'first',\n",
        "        'bbox_1': 'first',\n",
        "        'bbox_2': 'first',\n",
        "        'bbox_3': 'first',\n",
        "    }).reset_index()\n",
        "\n",
        "    # Add class information\n",
        "    grouped['detection_classes'] = class_info['class_id']\n",
        "    grouped['detection_classes_names'] = class_info['class_name']\n",
        "\n",
        "    return grouped\n",
        "\n",
        "def select_class_with_scores(group):\n",
        "    \"\"\"\n",
        "    Select class based on modal class, falling back to highest score for ties.\n",
        "    Returns both class ID and class name.\n",
        "    \"\"\"\n",
        "    # Get the value counts of classes\n",
        "    class_counts = group['detection_classes'].value_counts()\n",
        "\n",
        "    #print('class counts', class_counts)\n",
        "\n",
        "    # If there's a clear winner (one mode), use it\n",
        "    if len(class_counts) == 1 or class_counts.iloc[0] \u003e class_counts.iloc[1]:\n",
        "        class_id = group['detection_classes'].mode().iloc[0]\n",
        "    else:\n",
        "        # If there's a tie, look at highest score for each tied class\n",
        "        tied_classes = class_counts[class_counts == class_counts.iloc[0]].index\n",
        "        #print('tied classes', tied_classes)\n",
        "        class_max_scores = {\n",
        "            cls: group[group['detection_classes'] == cls]['detection_scores'].max()\n",
        "            for cls in tied_classes\n",
        "        }\n",
        "        #print('class max scores', class_max_scores)\n",
        "        class_id = max(class_max_scores.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    # Get corresponding class name\n",
        "    class_name = group[group['detection_classes'] == class_id]['detection_classes_names'].iloc[0]\n",
        "    #print('winner', pd.Series({'class_id': class_id, 'class_name': class_name}))\n",
        "    return pd.Series({'class_id': class_id, 'class_name': class_name})\n",
        "\n",
        "\n",
        "def apply_tracking(df,\n",
        "        search_range_x,\n",
        "        search_range_y,\n",
        "        memory):\n",
        "  \"\"\"Apply tracking to the dataframe.\n",
        "\n",
        "  Args:\n",
        "    df: The dataframe to apply tracking to.\n",
        "    search_range_x: The search range of pixels for tracking along x axis.\n",
        "    search_range_y: The search range of pixels for tracking along y axis.\n",
        "    memory: The frames memory for tracking.\n",
        "\n",
        "  Returns:\n",
        "    The tracking result dataframe.\n",
        "  \"\"\"\n",
        "  # Define the columns to link for tracking.\n",
        "  # Additional features that can be used are 'area', 'label', 'color',\n",
        "  # 'eccentricity', 'convex_area', 'mean_intensity-0', 'mean_intensity-1',\n",
        "  # 'mean_intensity-2', 'max_intensity-0', 'max_intensity-1', 'max_intensity-2',\n",
        "  # 'min_intensity-0',  'min_intensity-1', 'min_intensity-2'.\n",
        "  tracking_columns = [\n",
        "      'x',\n",
        "      'y',\n",
        "      'frame',\n",
        "      'bbox_0',\n",
        "      'bbox_1',\n",
        "      'bbox_2',\n",
        "      'bbox_3',\n",
        "      'major_axis_length',\n",
        "      'minor_axis_length',\n",
        "      'perimeter',\n",
        "  ]\n",
        "\n",
        "  # Perform the tracking operation on the specified columns\n",
        "  track_df = tp.link_df(df[tracking_columns], search_range=(search_range_y, search_range_x), memory=memory)\n",
        "\n",
        "  # Copy the additional columns from the original dataframe\n",
        "  additional_columns = [\n",
        "      'source_name',\n",
        "      'image_name',\n",
        "      'detection_scores',\n",
        "      'detection_classes_names',\n",
        "      'detection_classes',\n",
        "      'color',\n",
        "      'creation_time'\n",
        "  ]\n",
        "  track_df[additional_columns] = df[additional_columns]\n",
        "\n",
        "  track_df.drop(columns=['frame'], inplace=True)\n",
        "  track_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  return track_df\n",
        "\n",
        "\n",
        "def resize_bbox(y1, x1, y2, x2, old_height, old_width, new_height, new_width):\n",
        "    \"\"\"Resize bounding box coordinates based on new image size.\n",
        "\n",
        "    Args:\n",
        "        y1, x1, y2, x2 (int/float): Original bounding box coordinates.\n",
        "        old_height, old_width (int): Original image dimensions.\n",
        "        new_height, new_width (int): New image dimensions.\n",
        "\n",
        "    Returns:\n",
        "        (new_y1, new_x1, new_y2, new_x2): Rescaled bounding box coordinates.\n",
        "    \"\"\"\n",
        "    # Compute scale factors\n",
        "    scale_x = new_width / old_width\n",
        "    scale_y = new_height / old_height\n",
        "\n",
        "    # Scale bounding box coordinates\n",
        "    new_y1 = int(y1 * scale_y)\n",
        "    new_x1 = int(x1 * scale_x)\n",
        "    new_y2 = int(y2 * scale_y)\n",
        "    new_x2 = int(x2 * scale_x)\n",
        "\n",
        "    return new_y1, new_x1, new_y2, new_x2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XjfDEq--UlE"
      },
      "source": [
        "## Import and load pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ435YHN3Lr-"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget https://storage.googleapis.com/tf_model_garden/vision/\\\n",
        "waste_identification_ml/Detectron2_Jan2025_1024_1024.zip\n",
        "\n",
        "unzip Detectron2_Jan2025_1024_1024.zip \u003e /dev/null 2\u003e\u00261"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6mmyLsOJicF"
      },
      "source": [
        "## Import and load the labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RUzrh0uegqt"
      },
      "outputs": [],
      "source": [
        "LABELS_PATH = (\n",
        "    'models/official/projects/waste_identification_ml/pre_processing/'\n",
        "    'config/data/45_labels.csv'\n",
        ")\n",
        "\n",
        "labels = read_csv(LABELS_PATH)\n",
        "\n",
        "my_metadata = Metadata()\n",
        "my_metadata.set(thing_classes=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkdD8-QvGZ23"
      },
      "source": [
        "## Load all images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTMZWOCiECvL"
      },
      "outputs": [],
      "source": [
        "images_dir = \"/mydrive/circularnet/TestData/input-test-05022025\"\n",
        "images = glob.glob(os.path.join(images_dir, \"*\"))\n",
        "\n",
        "# Make sure that the files are sorted.\n",
        "images = natsort.natsorted(images)\n",
        "len(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HOCxVfejxlT"
      },
      "outputs": [],
      "source": [
        "# Prediction confidence score.\n",
        "PREDICTION_THRESHOLD = 0.70\n",
        "\n",
        "# The model is trained on 1024 x 1024 image dimensions.\n",
        "HEIGHT = 1024\n",
        "WIDTH = 1024\n",
        "\n",
        "# Object Tracking parameters.\n",
        "SEARCH_RANGE_X=150\n",
        "SEARCH_RANGE_Y=20\n",
        "MEMORY=1\n",
        "\n",
        "# Create a folder for saving prediction results.\n",
        "os.makedirs('prediction_folder', exist_ok=True)\n",
        "prediction_folder = os.path.join(os.getcwd(), 'prediction_folder')\n",
        "\n",
        "# Dimensions for tracking images.\n",
        "HEIGHT_TRACKING = 300\n",
        "WIDTH_TRACKING = 300\n",
        "\n",
        "# Create a folder to troubleshoot tracking results.\n",
        "os.makedirs('tracking', exist_ok=True)\n",
        "\n",
        "# Create a folder to save detected objects from Mask RCNN\n",
        "# accpording to categories\n",
        "output_dir = \"cropped_objects\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMRVdtYEN5bg"
      },
      "outputs": [],
      "source": [
        "# Initialize the Detectron2 configuration object\n",
        "cfg = get_cfg()\n",
        "\n",
        "# Load the model configuration from a YAML file.\n",
        "cfg.merge_from_file(\"config.yaml\")\n",
        "\n",
        "# Set the confidence threshold.\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = PREDICTION_THRESHOLD\n",
        "\n",
        "# Specify the path to the trained model weights.\n",
        "cfg.MODEL.WEIGHTS = \"model_final.pth\"\n",
        "\n",
        "cfg.MODEL.DEVICE = \"cuda\"\n",
        "\n",
        "# Create a predictor object using the configured model.\n",
        "predictor = DefaultPredictor(cfg)\n",
        "predictor.model.to(device)  # Ensure the model is on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3r73X-FGzz-"
      },
      "source": [
        "## Perform inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cm7IheKajzsW"
      },
      "outputs": [],
      "source": [
        "tracking_images = {}\n",
        "features_set = []\n",
        "\n",
        "\n",
        "for frame, image_path in tqdm.tqdm(enumerate(images, start=1)):\n",
        "  original_image = cv2.imread(image_path)\n",
        "  original_height, original_width = original_image.shape[:2]\n",
        "  resized_image = cv2.resize(\n",
        "    original_image,\n",
        "    (WIDTH, HEIGHT),\n",
        "    interpolation=cv2.INTER_AREA\n",
        "  )\n",
        "\n",
        "  # Perform inference.\n",
        "  results = predictor(resized_image)\n",
        "\n",
        "  # Implement class agnostic NMS.\n",
        "  results = convert_detections_to_instances(results)\n",
        "\n",
        "\n",
        "  # Extract the attributes from the prediction result.\n",
        "  fields = results[\"instances\"].to(\"cpu\").get_fields()\n",
        "  bboxes = fields[\"pred_boxes\"].tensor.numpy().astype(int)\n",
        "  if not len(bboxes):\n",
        "      continue\n",
        "\n",
        "  scores = fields[\"scores\"].numpy()\n",
        "  classes = fields[\"pred_classes\"].numpy()\n",
        "  masks = fields[\"pred_masks\"].numpy()\n",
        "\n",
        "  # Keep the predictions whose binary mask area \u003e 4000.\n",
        "  mask_areas = np.array([np.sum(i) for i in masks])\n",
        "  valid_indices = mask_areas \u003e 4000\n",
        "  bboxes = bboxes[valid_indices]\n",
        "  if not len(bboxes):\n",
        "      continue\n",
        "\n",
        "  scores = scores[valid_indices]\n",
        "  classes = classes[valid_indices]\n",
        "  masks = masks[valid_indices]\n",
        "\n",
        "  # Adjust the image size to ensure both dimensions are at least 1024\n",
        "  # for saving images with bbx and masks.\n",
        "  height_plot, width_plot = adjust_image_size(\n",
        "      original_image.shape[0], original_image.shape[1], 1024\n",
        "  )\n",
        "  image_plot = cv2.resize(\n",
        "      original_image,\n",
        "      (width_plot, height_plot),\n",
        "      interpolation=cv2.INTER_AREA,\n",
        "  )\n",
        "\n",
        "  # Rescale bounding boxes\n",
        "  scale_x = width_plot / WIDTH\n",
        "  scale_y = height_plot / HEIGHT\n",
        "  bboxes = (bboxes * [scale_x, scale_y, scale_x, scale_y]).astype(int)\n",
        "\n",
        "  # Rescale masks\n",
        "  if masks is not None:\n",
        "    resized_masks = np.array([\n",
        "        cv2.resize(mask.astype(\"uint8\"), (width_plot, height_plot), interpolation=cv2.INTER_NEAREST)\n",
        "        for mask in masks\n",
        "    ])\n",
        "  else:\n",
        "      resized_masks = None\n",
        "\n",
        "  # Convert predictions to Detectron2 visualization format\n",
        "  pred_boxes = Boxes(torch.tensor(bboxes, dtype=torch.float32))\n",
        "  pred_classes = torch.tensor(classes, dtype=torch.int64)\n",
        "  pred_scores = torch.tensor(scores, dtype=torch.float32)\n",
        "\n",
        "  predictions = {\n",
        "      \"pred_boxes\": pred_boxes,\n",
        "      \"scores\": pred_scores,\n",
        "      \"pred_classes\": pred_classes,\n",
        "  }\n",
        "\n",
        "  if resized_masks is not None:\n",
        "      predictions[\"pred_masks\"] = torch.tensor(resized_masks, dtype=torch.uint8)\n",
        "\n",
        "  # Save the prediction results as an image file with bbx and masks.\n",
        "  visualizer = Visualizer(\n",
        "    img_rgb=image_plot, metadata=my_metadata, scale=1\n",
        "  )\n",
        "  visualized_image = visualizer.draw_instance_predictions(\n",
        "      Instances((height_plot, width_plot),\n",
        "                **predictions\n",
        "      )\n",
        "  ).get_image()\n",
        "\n",
        "  final_image = Image.fromarray(cv2.hconcat([image_plot[:,:,::-1], visualized_image[:, :, ::-1]]))\n",
        "  final_image.save(f'prediction_folder/{os.path.basename(image_path)}')\n",
        "\n",
        "  # Create object tracking data.\n",
        "  tracking_image = cv2.resize(\n",
        "      original_image,\n",
        "      (WIDTH_TRACKING, HEIGHT_TRACKING),\n",
        "      interpolation=cv2.INTER_AREA,\n",
        "  )\n",
        "  tracking_images[os.path.basename(image_path)] = tracking_image\n",
        "\n",
        "  tracking_masks = np.array([\n",
        "        cv2.resize(\n",
        "            mask.astype(\"uint8\"),\n",
        "            (WIDTH_TRACKING, HEIGHT_TRACKING),\n",
        "            interpolation=cv2.INTER_NEAREST\n",
        "        ) for mask in masks\n",
        "  ])\n",
        "  # In case of connected masks, keep the biggest mask and fill the holes\n",
        "  # in case of incomplete detections by Mask RCNN.\n",
        "  tracking_masks = np.array([\n",
        "      dilated_largest_component(i) for i in tracking_masks]\n",
        "  )\n",
        "\n",
        "  # Crop objects from an image using masks for color detection.\n",
        "  cropped_objects = [\n",
        "      np.where(np.expand_dims(i, -1), image_plot[:,:,::-1], 0)\n",
        "      for i in resized_masks\n",
        "  ]\n",
        "\n",
        "  # Perform color detection using clustering approach.\n",
        "  dominant_colors = [\n",
        "        *map(\n",
        "            color_and_property_extractor.find_dominant_color, cropped_objects\n",
        "        )\n",
        "  ]\n",
        "  generic_color_names = color_and_property_extractor.get_generic_color_name(dominant_colors)\n",
        "\n",
        "  # Extract features.\n",
        "  features = extract_properties(\n",
        "        tracking_image, tracking_masks\n",
        "    )\n",
        "  features[\"source_name\"] = os.path.basename(os.path.dirname(image_path))\n",
        "  features[\"image_name\"] = os.path.basename(image_path)\n",
        "  features[\"creation_time\"] = get_image_creation_time(image_path)\n",
        "  features[\"frame\"] = frame\n",
        "  features[\"detection_scores\"] = list(scores)\n",
        "  features[\"detection_classes\"] = list(classes)\n",
        "  features[\"detection_classes_names\"] = [labels[i] for i in list(classes)]\n",
        "  features[\"color\"] = generic_color_names\n",
        "  features_set.append(features)\n",
        "\n",
        "\n",
        "if features_set:\n",
        "  features_df = pd.concat(features_set, ignore_index=True)\n",
        "  tracking_features = apply_tracking(\n",
        "      features_df,\n",
        "      search_range_x=SEARCH_RANGE_X,\n",
        "      search_range_y=SEARCH_RANGE_Y,\n",
        "      memory=MEMORY\n",
        "  )\n",
        "  agg_features = process_tracking_result(tracking_features)\n",
        "  counts = agg_features.groupby(\"detection_classes_names\").size()\n",
        "  counts.to_frame().to_csv(os.path.join(os.getcwd(), \"count.csv\"))\n",
        "  print(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcEjtdLL-CnZ"
      },
      "source": [
        "## Visualize Object Tracking  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yZOHwsS5GLL"
      },
      "outputs": [],
      "source": [
        "CIRCLE_RADIUS =7\n",
        "CIRCLE_THICKNESS = 3\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "fontScale = 1\n",
        "color = (255, 0, 0)\n",
        "groups = tracking_features.groupby('image_name')\n",
        "\n",
        "for name, group in groups:\n",
        "  img = tracking_images[name].copy()\n",
        "  for k in range(len(group)):\n",
        "    cv2.circle(img,\n",
        "               (int(group.iloc[k]['x']),int(group.iloc[k]['y'])),\n",
        "               CIRCLE_RADIUS,\n",
        "               (255,133,233),\n",
        "               -1\n",
        "    )\n",
        "    cv2.putText(img,\n",
        "                str(int(group.iloc[k]['particle'])),\n",
        "                 (int(group.iloc[k]['x']), int(group.iloc[k]['y'])),\n",
        "                font,\n",
        "                fontScale,\n",
        "                color,\n",
        "                2,\n",
        "                cv2.LINE_AA\n",
        "    )\n",
        "\n",
        "  cv2.imwrite(os.path.join('tracking',name), img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW8GKKl7-Voy"
      },
      "source": [
        "## Visualize Predictions by Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5neH4_tsdZO"
      },
      "outputs": [],
      "source": [
        "if not agg_features.empty:\n",
        "  for group_name, df in tqdm.tqdm(agg_features.groupby(\"detection_classes_names\")):\n",
        "    os.makedirs(f'{output_dir}/{group_name}', exist_ok=True)\n",
        "\n",
        "    for row in df.itertuples(index=False):\n",
        "      # Get the image\n",
        "      image = cv2.imread(os.path.join(images_dir, row.image_name))\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      new_h, new_w = image.shape[0], image.shape[1]\n",
        "\n",
        "      # Get the bounding box and resize it\n",
        "      y1, x1, y2, x2 = row.bbox_0, row.bbox_1, row.bbox_2, row.bbox_3\n",
        "      new_bbox = resize_bbox(y1, x1, y2, x2, HEIGHT_TRACKING, WIDTH_TRACKING, new_h, new_w)\n",
        "\n",
        "      # Include the score in the filename\n",
        "      score = row.detection_scores if hasattr(row, 'detection_scores') else 0.0\n",
        "      name = f'{os.path.splitext(row.image_name)[0]}_{row.particle}_{score:.2f}.png'\n",
        "\n",
        "      # Save the cropped image\n",
        "      cv2.imwrite(f'{output_dir}/{row.detection_classes_names}/{name}',\n",
        "                 image[new_bbox[0]:new_bbox[2], new_bbox[1]:new_bbox[3]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgmZvRqCDrnh"
      },
      "source": [
        "## Copying folders to my Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgmX0qaLDuj9"
      },
      "outputs": [],
      "source": [
        "destination_folder = '/mydrive/circularnet/TestModel'\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Function to safely copy directory, removing destination first if it exists\n",
        "def copytree_replace(src, dst):\n",
        "  if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "  shutil.copytree(src, dst)\n",
        "\n",
        "# Function to safely copy file, overwriting if it exists\n",
        "def copy_replace(src, dst):\n",
        "  if os.path.exists(dst):\n",
        "    os.remove(dst)\n",
        "  shutil.copy(src, dst)\n",
        "\n",
        "copytree_replace(os.path.join(os.getcwd(), \"prediction_folder\"), os.path.join(destination_folder, \"prediction_folder\"))\n",
        "copytree_replace(os.path.join(os.getcwd(), \"cropped_objects\"),os.path.join(destination_folder, \"cropped_objects\"))\n",
        "copytree_replace(os.path.join(os.getcwd(), \"tracking\"),os.path.join(destination_folder, \"tracking\"))\n",
        "copy_replace(os.path.join(os.getcwd(), \"count.csv\"),os.path.join(destination_folder, \"count.csv\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
