{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtlIRiNXWlQ0"
      },
      "source": [
        "# Waste identification with instance segmentation in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohoMgYgXWsIO"
      },
      "source": [
        "This Colab notebook demonstrates an end-to-end pipeline for object detection, feature extraction, object tracking, and data aggregation using the Mask R-CNN model from TensorFlow Model Garden.\n",
        "Key Steps in the Notebook:\n",
        "\n",
        "\n",
        "\n",
        "*   Object Detection and segmentation – Detect objects in a set of images using Mask R-CNN.\n",
        "*   Feature Extraction \u0026 Tracking – Extract object features and track them across multiple frames to eliminate duplicate counts.\n",
        "*   Color Detection – Identify the color of each detected object.\n",
        "*   Postprocessing – Aggregate tracking results and apply filtering to reduce false positives and false negatives.\n",
        "*   Save detection and tracking results.\n",
        "*   Push the final object count to a BigQuery table, which can be connected to a Looker dashboard for visualization in Google Cloud Platform (GCP).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PKG9z4VYPEs"
      },
      "source": [
        "To finish this task, a proper path for the saved models and images need to be provided. The path to the labels on which the models are trained is in the waste_identification_ml directory inside the Tensorflow Model Garden repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2J5IWCgAOxC"
      },
      "source": [
        "This notebook will output 3 folders and 1 csv file :\n",
        "\n",
        "\n",
        "*   **prediction_folder** : Will contain prediction results with bbox and masks.\n",
        "*   **tracking** : Will contain tracking visualization.\n",
        "*   **cropped_objects** : Will contain category level detected objects.\n",
        "*   **count.csv** : Will contain the individual counts of each category.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ELUFMVDDAopS"
      },
      "outputs": [],
      "source": [
        "#@title Imports and Setup\n",
        "\n",
        "!pip install -q trackpy\n",
        "\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "from typing import Any, TypedDict, Callable\n",
        "import cv2\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import natsort\n",
        "import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "import pandas as pd\n",
        "import skimage\n",
        "import datetime\n",
        "import trackpy as tp\n",
        "import shutil\n",
        "\n",
        "logging.disable(logging.WARNING)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5_XxVSFATYZ"
      },
      "outputs": [],
      "source": [
        "# Connect to Google drive if your data is stored there.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "try:\n",
        "  !ln -s /content/gdrive/My\\ Drive/ /mydrive\n",
        "  print('Successful')\n",
        "except Exception as e:\n",
        "  print(e)\n",
        "  print('Not successful')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAQ8ymV-Cgqp"
      },
      "outputs": [],
      "source": [
        "# # Connect to GCP bucket if your data is store there and copy them locally.\n",
        "# !gcloud init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_77YK3a_BCg_"
      },
      "source": [
        "To visualize the images with the proper detected boxes and segmentation masks, we will use the TensorFlow Object Detection API. To install it we will clone the repo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhk_NujKO0mb"
      },
      "outputs": [],
      "source": [
        "# Clone the tensorflow models repository.\n",
        "!git clone --depth 1 https://github.com/tensorflow/models 2\u003e/dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1dYyG55BtWb"
      },
      "outputs": [],
      "source": [
        "sys.path.append('models/research/')\n",
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "sys.path.append('models/official/projects/waste_identification_ml/model_inference/')\n",
        "import color_and_property_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GO488S78_2GJ"
      },
      "outputs": [],
      "source": [
        "#@title Utilities\n",
        "\n",
        "_PROPERTIES = (\n",
        "    'area',\n",
        "    'bbox',\n",
        "    'convex_area',\n",
        "    'bbox_area',\n",
        "    'major_axis_length',\n",
        "    'minor_axis_length',\n",
        "    'eccentricity',\n",
        "    'centroid',\n",
        "    'label',\n",
        "    'mean_intensity',\n",
        "    'max_intensity',\n",
        "    'min_intensity',\n",
        "    'perimeter'\n",
        ")\n",
        "\n",
        "\n",
        "class ItemDict(TypedDict):\n",
        "  id: int\n",
        "  name: str\n",
        "  supercategory: str\n",
        "\n",
        "\n",
        "def load_model(model_path: str) -\u003e Callable:\n",
        "    \"\"\"Loads a TensorFlow SavedModel and returns a function for making predictions.\n",
        "\n",
        "    Args:\n",
        "      model_path: Path to the TensorFlow SavedModel.\n",
        "\n",
        "    Returns:\n",
        "      A function that can be used to make predictions.\n",
        "    \"\"\"\n",
        "    try:\n",
        "      print('loading model...')\n",
        "      model = tf.saved_model.load(model_path)\n",
        "      print('model loaded!')\n",
        "      detection_fn = model.signatures['serving_default']\n",
        "      return detection_fn\n",
        "    except (OSError, ValueError, KeyError) as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def perform_detection(model: Callable, image: np.ndarray) -\u003e dict[str, np.ndarray]:\n",
        "    \"\"\"Perform Mask R-CNN object detection on an image using the specified model.\n",
        "\n",
        "    Args:\n",
        "        model: A function that can be used to make predictions.\n",
        "        image: A NumPy array representing the image to be processed.\n",
        "\n",
        "    Returns:\n",
        "        Detection results, where keys are output names and values are NumPy arrays.\n",
        "    \"\"\"\n",
        "    detection_results = model(image)\n",
        "    detection_results = {key: value.numpy() for key, value in detection_results.items()}\n",
        "    return detection_results\n",
        "\n",
        "\n",
        "def _read_csv_to_list(file_path: str) -\u003e list[str]:\n",
        "  \"\"\"Reads a CSV file and returns its contents as a list.\n",
        "\n",
        "  This function reads the given CSV file, skips the header, and assumes\n",
        "  there is only one column in the CSV. It returns the contents as a list of\n",
        "  strings.\n",
        "\n",
        "  Args:\n",
        "      file_path: The path to the CSV file.\n",
        "\n",
        "  Returns:\n",
        "      The contents of the CSV file as a list of strings.\n",
        "  \"\"\"\n",
        "  data_list = []\n",
        "  with open(file_path, 'r') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "      data_list.append(row[0])  # Assuming there is only one column in the CSV\n",
        "  return data_list\n",
        "\n",
        "\n",
        "def _categories_dictionary(objects: list[str]) -\u003e dict[int, ItemDict]:\n",
        "  \"\"\"This function takes a list of objects and returns a dictionaries.\n",
        "\n",
        "  A dictionary of objects, where each object is represented by a dictionary\n",
        "  with the following keys:\n",
        "    - id: The ID of the object.\n",
        "    - name: The name of the object.\n",
        "    - supercategory: The supercategory of the object.\n",
        "\n",
        "  Args:\n",
        "    objects: A list of strings, where each string is the name of an\n",
        "      object.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of two dictionaries, as described above.\n",
        "  \"\"\"\n",
        "  category_index = {}\n",
        "  for num, obj_name in enumerate(objects, start=1):\n",
        "    obj_dict = {'id': num, 'name': obj_name, 'supercategory': 'objects'}\n",
        "    category_index[num] = obj_dict\n",
        "  return category_index\n",
        "\n",
        "\n",
        "def load_labels(labels_path: str) -\u003e tuple[list[str], dict[int, ItemDict]]:\n",
        "    \"\"\"\n",
        "    Load label mappings from a CSV file and generate category indices.\n",
        "\n",
        "    Args:\n",
        "        labels_path (str): Path to the CSV file containing label mappings.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[int, dict], Dict[int, dict]]:\n",
        "        - A dictionary mapping category IDs to label details.\n",
        "        - A processed category index dictionary.\n",
        "    \"\"\"\n",
        "    labels = _read_csv_to_list(labels_path)\n",
        "    category_index = _categories_dictionary(labels)\n",
        "    return labels, category_index\n",
        "\n",
        "\n",
        "def preprocess_image(path: str, height: int, width: int) -\u003e tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load an image from a file into a NumPy array, resize it, and expand dimensions for batch processing.\n",
        "\n",
        "    Args:\n",
        "        path: The file path to the image.\n",
        "        height: Desired height of the resized image.\n",
        "        width: Desired width of the resized image.\n",
        "\n",
        "    Returns:\n",
        "        original_image: The original image with shape (original_height, original_width, 3).\n",
        "        resized_image: The resized image with shape (1, height, width, 3), suitable for model input.\n",
        "    \"\"\"\n",
        "    original_image = cv2.imread(path)\n",
        "    if original_image is None:\n",
        "        raise FileNotFoundError(f\"Image not found at path: {path}\")\n",
        "\n",
        "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "    resized_image = cv2.resize(original_image, (width, height), interpolation=cv2.INTER_AREA)\n",
        "    resized_image = np.expand_dims(resized_image, axis=0)\n",
        "\n",
        "    return original_image, resized_image\n",
        "\n",
        "\n",
        "def filter_detection(results: dict[str, np.ndarray], valid_indices: np.ndarray) -\u003e dict[str, np.ndarray]:\n",
        "  \"\"\"Filter the detection results based on the valid indices.\n",
        "\n",
        "  Args:\n",
        "    results: The detection results from the model.\n",
        "    valid_indices: The indices of the valid detections.\n",
        "\n",
        "  Returns:\n",
        "    The filtered detection results.\n",
        "  \"\"\"\n",
        "  if np.array(valid_indices).dtype == bool:\n",
        "    new_num_detections = int(np.sum(valid_indices))\n",
        "  else:\n",
        "    new_num_detections = len(valid_indices)\n",
        "\n",
        "  # Define the keys to filter\n",
        "  keys_to_filter = [\n",
        "      'detection_masks',\n",
        "      'detection_masks_resized',\n",
        "      'detection_masks_reframed',\n",
        "      'detection_classes',\n",
        "      'detection_boxes',\n",
        "      'normalized_boxes',\n",
        "      'detection_scores',\n",
        "      'detection_classes_names',\n",
        "  ]\n",
        "\n",
        "  # Apply filtering to the specified keys\n",
        "  filtered_output = {}\n",
        "\n",
        "  for key in keys_to_filter:\n",
        "    if key in results:\n",
        "      if key == 'detection_masks':\n",
        "        filtered_output[key] = results[key][:, valid_indices, :, :]\n",
        "      elif key in ['detection_masks_resized', 'detection_masks_reframed']:\n",
        "        filtered_output[key] = results[key][valid_indices, :, :]\n",
        "      elif key in ['detection_boxes', 'normalized_boxes']:\n",
        "        filtered_output[key] = results[key][:, valid_indices, :]\n",
        "      elif key in ['detection_classes', 'detection_scores', 'detection_classes_names']:\n",
        "        filtered_output[key] = results[key][:, valid_indices]\n",
        "  filtered_output['num_detections'] = np.array([new_num_detections])\n",
        "\n",
        "  return filtered_output\n",
        "\n",
        "\n",
        "\n",
        "def reframe_masks(results: dict[str, np.ndarray], boxes: str, height: int, width: int) -\u003e np.ndarray:\n",
        "  \"\"\"Reframe the masks to an image size.\n",
        "\n",
        "  Args:\n",
        "    results: The detection results from the model.\n",
        "    boxes: The detection boxes.\n",
        "    height: The height of the original image.\n",
        "    width: The width of the original image.\n",
        "\n",
        "  Returns:\n",
        "    The reframed masks.\n",
        "  \"\"\"\n",
        "  detection_masks = results['detection_masks'][0]\n",
        "  detection_boxes = results[boxes][0]\n",
        "  detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "      detection_masks, detection_boxes, height, width\n",
        "  )\n",
        "  detection_masks_reframed = tf.cast(detection_masks_reframed \u003e 0.5, np.uint8)\n",
        "  detection_masks_reframed = detection_masks_reframed.numpy()\n",
        "  return detection_masks_reframed\n",
        "\n",
        "\n",
        "def _calculate_area(mask: np.ndarray) -\u003e int:\n",
        "  \"\"\"Calculate the area of the mask.\n",
        "\n",
        "  Args:\n",
        "    mask: The mask to calculate the area of.\n",
        "\n",
        "  Returns:\n",
        "    The area of the mask.\n",
        "  \"\"\"\n",
        "  return np.sum(mask)\n",
        "\n",
        "\n",
        "def _calculate_iou(mask1: np.ndarray, mask2: np.ndarray) -\u003e float:\n",
        "  \"\"\"Calculate the intersection over union (IoU) between two masks.\n",
        "\n",
        "  Args:\n",
        "    mask1: The first mask.\n",
        "    mask2: The second mask.\n",
        "\n",
        "  Returns:\n",
        "    The intersection over union (IoU) between the two masks.\n",
        "  \"\"\"\n",
        "  intersection = np.logical_and(mask1, mask2).sum()\n",
        "  union = np.logical_or(mask1, mask2).sum()\n",
        "  return intersection / union if union != 0 else 0\n",
        "\n",
        "\n",
        "def _is_contained(mask1: np.ndarray, mask2: np.ndarray) -\u003e bool:\n",
        "  \"\"\"Check if mask1 is entirely contained within mask2.\n",
        "\n",
        "  Args:\n",
        "    mask1: The first mask.\n",
        "    mask2: The second mask.\n",
        "\n",
        "  Returns:\n",
        "    True if mask1 is entirely contained within mask2, False otherwise.\n",
        "  \"\"\"\n",
        "  return np.array_equal(np.logical_and(mask1, mask2), mask1)\n",
        "\n",
        "\n",
        "def filter_masks(masks: np.ndarray, iou_threshold=0.8, area_threshold=None) -\u003e np.ndarray:\n",
        "  \"\"\"Filter the overlapping masks.\n",
        "\n",
        "  Filter the masks based on the area and intersection over union (IoU).\n",
        "\n",
        "  Args:\n",
        "    masks: The masks to filter.\n",
        "    iou_threshold: The threshold for the intersection over union (IoU) between\n",
        "      two masks.\n",
        "    area_threshold: The threshold for the area of the mask.\n",
        "\n",
        "  Returns:\n",
        "    The indices of the unique masks.\n",
        "  \"\"\"\n",
        "  # Calculate the area for each mask\n",
        "  areas = np.array([_calculate_area(mask) for mask in masks])\n",
        "\n",
        "  # Sort the masks based on area in descending order\n",
        "  sorted_indices = np.argsort(areas)[::-1]\n",
        "  sorted_masks = masks[sorted_indices]\n",
        "  sorted_areas = areas[sorted_indices]\n",
        "\n",
        "  unique_indices = []\n",
        "\n",
        "  for i, mask in enumerate(sorted_masks):\n",
        "    if (area_threshold is not None and sorted_areas[i] \u003e area_threshold) or sorted_areas[i] \u003c 4000:\n",
        "      continue\n",
        "\n",
        "    keep = True\n",
        "    for j in range(i):\n",
        "      if _calculate_iou(mask, sorted_masks[j]) \u003e iou_threshold or _is_contained(\n",
        "          mask, sorted_masks[j]\n",
        "      ):\n",
        "        keep = False\n",
        "        break\n",
        "    if keep:\n",
        "      unique_indices.append(sorted_indices[i])\n",
        "\n",
        "  return unique_indices\n",
        "\n",
        "\n",
        "def adjust_image_size(height: int, width: int, min_size: int) -\u003e tuple[int, int]:\n",
        "  \"\"\"Adjust the image size to ensure both dimensions are at least 1024.\n",
        "\n",
        "  Args:\n",
        "    height: The height of the image.\n",
        "    width: The width of the image.\n",
        "    min_size: Minimum size of the image dimension needed.\n",
        "\n",
        "  Returns:\n",
        "    The adjusted height and width of the image.\n",
        "  \"\"\"\n",
        "  if height \u003c min_size or width \u003c min_size:\n",
        "    return height, width\n",
        "\n",
        "  # Calculate the scale factor to ensure both dimensions remain at least 1024\n",
        "  scale_factor = min(height / min_size, width / min_size)\n",
        "\n",
        "  new_height = int(height / scale_factor)\n",
        "  new_width = int(width / scale_factor)\n",
        "\n",
        "  return new_height, new_width\n",
        "\n",
        "\n",
        "def display_bbox_masks_labels(\n",
        "    result: dict[Any, np.ndarray],\n",
        "    image: np.ndarray,\n",
        "    category_index: dict[int, dict[str, str]],\n",
        "    threshold: float,\n",
        ") -\u003e None:\n",
        "  \"\"\"Saves an image with visualized bounding boxes, labels, and masks.\n",
        "\n",
        "  This function takes the output from Mask R-CNN, copies the original image,\n",
        "  and applies visualizations of detection boxes, classes, and scores.\n",
        "  If available, it also applies segmentation masks. The result is an image that\n",
        "  juxtaposes the original with the annotated version, saved to the specified\n",
        "  folder.\n",
        "\n",
        "  Args:\n",
        "    result: The output from theMask RCNN model, expected to contain detection\n",
        "      boxes, classes, scores, reframed detection masks, etc.\n",
        "    image: The original image as a numpy array.\n",
        "    file_name: The filename for saving the output image.\n",
        "    folder: The folder path where the output image will be saved.\n",
        "    category_index: A dictionary mapping class IDs to class labels.\n",
        "    threshold: Value between 0 and 1 to filter out the prediction results.\n",
        "  \"\"\"\n",
        "  image_new = image.copy()\n",
        "  image_new = cv2.cvtColor(image_new, cv2.COLOR_BGR2RGB)\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_new,\n",
        "      result['normalized_boxes'][0],\n",
        "      (result['detection_classes'][0] + 0).astype(int),\n",
        "      result['detection_scores'][0],\n",
        "      category_index=category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=100,\n",
        "      min_score_thresh=threshold,\n",
        "      agnostic_mode=False,\n",
        "      instance_masks=result.get('detection_masks_reframed', None),\n",
        "      line_thickness=4,\n",
        "  )\n",
        "  return image_new\n",
        "\n",
        "\n",
        "def save_bbox_masks_labels(\n",
        "    result: dict[Any, np.ndarray],\n",
        "    image: np.ndarray,\n",
        "    file_name: str,\n",
        "    folder: str,\n",
        "    category_index: dict[int, dict[str, str]],\n",
        "    threshold: float,\n",
        ") -\u003e None:\n",
        "  \"\"\"Saves an image with visualized bounding boxes, labels, and masks.\n",
        "\n",
        "  This function takes the output from Mask R-CNN, copies the original image,\n",
        "  and applies visualizations of detection boxes, classes, and scores.\n",
        "  If available, it also applies segmentation masks. The result is an image that\n",
        "  juxtaposes the original with the annotated version, saved to the specified\n",
        "  folder.\n",
        "\n",
        "  Args:\n",
        "    result: The output from theMask RCNN model, expected to contain detection\n",
        "      boxes, classes, scores, reframed detection masks, etc.\n",
        "    image: The original image as a numpy array.\n",
        "    file_name: The filename for saving the output image.\n",
        "    folder: The folder path where the output image will be saved.\n",
        "    category_index: A dictionary mapping class IDs to class labels.\n",
        "    threshold: Value between 0 and 1 to filter out the prediction results.\n",
        "  \"\"\"\n",
        "  image_new = image.copy()\n",
        "  viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_new,\n",
        "      result['normalized_boxes'][0],\n",
        "      (result['detection_classes'][0] + 0).astype(int),\n",
        "      result['detection_scores'][0],\n",
        "      category_index=category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=100,\n",
        "      min_score_thresh=threshold,\n",
        "      agnostic_mode=False,\n",
        "      instance_masks=result.get('detection_masks_reframed', None),\n",
        "      line_thickness=4,\n",
        "  )\n",
        "\n",
        "  concatenated_image = np.concatenate((image, image_new), axis=1)\n",
        "  concatenated_image = Image.fromarray(concatenated_image)\n",
        "  concatenated_image.save(os.path.join(folder, file_name))\n",
        "\n",
        "\n",
        "def dilated_largest_component(mask: np.ndarray) -\u003e np.ndarray:\n",
        "    \"\"\"Extracts the largest connected component and fills holes.\n",
        "\n",
        "    Args:\n",
        "        mask: Input binary mask (2D numpy array).\n",
        "\n",
        "    Returns:\n",
        "        Binary mask of the largest connected component.\n",
        "    \"\"\"\n",
        "    mask = mask.astype(np.uint8)*255\n",
        "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
        "    largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
        "    largest_component_mask = np.zeros(mask.shape, dtype=\"uint8\")\n",
        "    largest_component_mask[labels == largest_label] = 1\n",
        "    largest_component_mask = ndimage.binary_fill_holes(largest_component_mask).astype(int)\n",
        "    return largest_component_mask\n",
        "\n",
        "\n",
        "def extract_properties(image, results, masks):\n",
        "  \"\"\"Extract properties of the mask.\n",
        "\n",
        "  Args:\n",
        "    image: Corresponding image of the mask.\n",
        "    results: The detection results from the model.\n",
        "    masks: The masks to extract properties from.\n",
        "\n",
        "  Returns:\n",
        "    The extracted properties.\n",
        "  \"\"\"\n",
        "  list_of_df = []\n",
        "  for mask in results[masks]:\n",
        "    mask = np.where(mask, 1, 0)\n",
        "    df = pd.DataFrame(\n",
        "        skimage.measure.regionprops_table(mask, intensity_image=image, properties=_PROPERTIES)\n",
        "    )\n",
        "    list_of_df.append(df)\n",
        "  features = pd.concat(list_of_df, ignore_index=True)\n",
        "  features.rename(\n",
        "      columns={\n",
        "          'centroid-0': 'y',\n",
        "          'centroid-1': 'x',\n",
        "          'bbox-0': 'bbox_0',\n",
        "          'bbox-1': 'bbox_1',\n",
        "          'bbox-2': 'bbox_2',\n",
        "          'bbox-3': 'bbox_3',\n",
        "      },\n",
        "      inplace=True,\n",
        "  )\n",
        "  return features\n",
        "\n",
        "\n",
        "def get_image_creation_time(image_path):\n",
        "  \"\"\"\n",
        "  Retrieves the creation time of an image, trying multiple methods.\n",
        "\n",
        "  Args:\n",
        "    image_path: The path to the image file.\n",
        "\n",
        "  Returns:\n",
        "    A string representing the creation time in the format \"%Y-%m-%d %H:%M:%S\" if\n",
        "    found, otherwise returns \"Creation time not found\".\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # 1. Try EXIF data (if available)\n",
        "    image = Image.open(image_path)\n",
        "    exif_data = image._getexif()\n",
        "    if exif_data:\n",
        "      datetime_tag_id = 36867  # Tag ID for \"DateTimeOriginal\"\n",
        "      datetime_str = exif_data.get(datetime_tag_id)\n",
        "      if datetime_str:\n",
        "        datetime_obj = datetime.datetime.strptime(datetime_str, \"%Y:%m:%d %H:%M:%S\")\n",
        "        return datetime_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # 2. Try file modification time (less accurate, but better than nothing)\n",
        "    file_modified_time = os.path.getmtime(image_path)\n",
        "    datetime_obj = datetime.datetime.fromtimestamp(file_modified_time)\n",
        "    return datetime_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    return \"Image not found\"\n",
        "  except Exception as e:\n",
        "    return f\"Error: {e}\"\n",
        "\n",
        "\n",
        "def apply_tracking(df,\n",
        "        search_range_x,\n",
        "        search_range_y,\n",
        "        memory):\n",
        "  \"\"\"Apply tracking to the dataframe.\n",
        "\n",
        "  Args:\n",
        "    df: The dataframe to apply tracking to.\n",
        "    search_range_x: The search range of pixels for tracking along x axis.\n",
        "    search_range_y: The search range of pixels for tracking along y axis.\n",
        "    memory: The frames memory for tracking.\n",
        "\n",
        "  Returns:\n",
        "    The tracking result dataframe.\n",
        "  \"\"\"\n",
        "  # Define the columns to link for tracking.\n",
        "  # Additional features that can be used are 'area', 'label', 'color',\n",
        "  # 'eccentricity', 'convex_area', 'mean_intensity-0', 'mean_intensity-1',\n",
        "  # 'mean_intensity-2', 'max_intensity-0', 'max_intensity-1', 'max_intensity-2',\n",
        "  # 'min_intensity-0',  'min_intensity-1', 'min_intensity-2'.\n",
        "  tracking_columns = [\n",
        "      'x',\n",
        "      'y',\n",
        "      'frame',\n",
        "      'bbox_0',\n",
        "      'bbox_1',\n",
        "      'bbox_2',\n",
        "      'bbox_3',\n",
        "      'major_axis_length',\n",
        "      'minor_axis_length',\n",
        "      'perimeter',\n",
        "  ]\n",
        "\n",
        "  # Perform the tracking operation on the specified columns\n",
        "  track_df = tp.link_df(df[tracking_columns], search_range=(search_range_y, search_range_x), memory=memory)\n",
        "\n",
        "  # Copy the additional columns from the original dataframe\n",
        "  additional_columns = [\n",
        "      'source_name',\n",
        "      'image_name',\n",
        "      'detection_scores',\n",
        "      'detection_classes_names',\n",
        "      'detection_classes',\n",
        "      'color',\n",
        "      'creation_time'\n",
        "  ]\n",
        "  track_df[additional_columns] = df[additional_columns]\n",
        "\n",
        "  track_df.drop(columns=['frame'], inplace=True)\n",
        "  track_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  return track_df\n",
        "\n",
        "\n",
        "def process_tracking_result(df):\n",
        "    \"\"\"Process the tracking result dataframe.\n",
        "\n",
        "    Args:\n",
        "      df: Dataframe to be aggregated.\n",
        "\n",
        "    Returns:\n",
        "      Processed dataframe.\n",
        "    \"\"\"\n",
        "    # Get class information with the new include_groups parameter\n",
        "    class_info = df.groupby('particle', as_index=False).apply(\n",
        "        select_class_with_scores,\n",
        "        include_groups=False\n",
        "    )\n",
        "\n",
        "    grouped = df.groupby('particle').agg({\n",
        "        'source_name': 'first',\n",
        "        'image_name': 'first',\n",
        "        'detection_scores': 'max',\n",
        "        'creation_time': 'first',\n",
        "        'bbox_0': 'first',\n",
        "        'bbox_1': 'first',\n",
        "        'bbox_2': 'first',\n",
        "        'bbox_3': 'first',\n",
        "    }).reset_index()\n",
        "\n",
        "    # Add class information\n",
        "    grouped['detection_classes'] = class_info['class_id']\n",
        "    grouped['detection_classes_names'] = class_info['class_name']\n",
        "\n",
        "    return grouped\n",
        "\n",
        "def select_class_with_scores(group):\n",
        "    \"\"\"\n",
        "    Select class based on modal class, falling back to highest score for ties.\n",
        "    Returns both class ID and class name.\n",
        "    \"\"\"\n",
        "    # Get the value counts of classes\n",
        "    class_counts = group['detection_classes'].value_counts()\n",
        "\n",
        "    # If there's a clear winner (one mode), use it\n",
        "    if len(class_counts) == 1 or class_counts.iloc[0] \u003e class_counts.iloc[1]:\n",
        "        class_id = group['detection_classes'].mode().iloc[0]\n",
        "    else:\n",
        "        # If there's a tie, look at highest score for each tied class\n",
        "        tied_classes = class_counts[class_counts == class_counts.iloc[0]].index\n",
        "        class_max_scores = {\n",
        "            cls: group[group['detection_classes'] == cls]['detection_scores'].max()\n",
        "            for cls in tied_classes\n",
        "        }\n",
        "        class_id = max(class_max_scores.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    # Get corresponding class name\n",
        "    class_name = group[group['detection_classes'] == class_id]['detection_classes_names'].iloc[0]\n",
        "    return pd.Series({'class_id': class_id, 'class_name': class_name})\n",
        "\n",
        "\n",
        "def resize_bbox(y1, x1, y2, x2, old_height, old_width, new_height, new_width):\n",
        "    \"\"\"Resize bounding box coordinates based on new image size.\n",
        "\n",
        "    Args:\n",
        "        y1, x1, y2, x2 (int/float): Original bounding box coordinates.\n",
        "        old_height, old_width (int): Original image dimensions.\n",
        "        new_height, new_width (int): New image dimensions.\n",
        "\n",
        "    Returns:\n",
        "        (new_y1, new_x1, new_y2, new_x2): Rescaled bounding box coordinates.\n",
        "    \"\"\"\n",
        "    # Compute scale factors\n",
        "    scale_x = new_width / old_width\n",
        "    scale_y = new_height / old_height\n",
        "\n",
        "    # Scale bounding box coordinates\n",
        "    new_y1 = int(y1 * scale_y)\n",
        "    new_x1 = int(x1 * scale_x)\n",
        "    new_y2 = int(y2 * scale_y)\n",
        "    new_x2 = int(x2 * scale_x)\n",
        "\n",
        "    return new_y1, new_x1, new_y2, new_x2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XjfDEq--UlE"
      },
      "source": [
        "## Import and load pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ435YHN3Lr-"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget https://storage.googleapis.com/tf_model_garden/vision/\\\n",
        "waste_identification_ml/Jan2025_ver2_merged_1024_1024.zip -q\n",
        "\n",
        "unzip Jan2025_ver2_merged_1024_1024.zip \u003e /dev/null 2\u003e\u00261"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMRVdtYEN5bg"
      },
      "outputs": [],
      "source": [
        "detection_fn = load_model('Jan2025_ver2_merged_1024_1024/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6mmyLsOJicF"
      },
      "source": [
        "## Load label map data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM2A29OrJqaU"
      },
      "source": [
        "Label maps correspond index numbers to category names, so that when our convolution network predicts 5, we know that this corresponds to airplane. Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine.\n",
        "\n",
        "We will load our labels from the same repository that we loaded the TF Object Detection API from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RUzrh0uegqt"
      },
      "outputs": [],
      "source": [
        "LABELS_PATH = (\n",
        "    'models/official/projects/waste_identification_ml/pre_processing/'\n",
        "    'config/data/45_labels.csv'\n",
        ")\n",
        "\n",
        "labels, category_index = load_labels(LABELS_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkdD8-QvGZ23"
      },
      "source": [
        "## Load all images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTMZWOCiECvL"
      },
      "outputs": [],
      "source": [
        "images_dir = \"/mydrive/circularnet/TestData/input-test-05022025\"\n",
        "images = glob.glob(os.path.join(images_dir, \"*\"))\n",
        "\n",
        "# Make sure that the files are sorted.\n",
        "images = natsort.natsorted(images)\n",
        "len(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7ZS7gHgGk9f"
      },
      "outputs": [],
      "source": [
        "# The model is trained on 1024 x 1024 image dimensions.\n",
        "HEIGHT = 1024\n",
        "WIDTH = 1024\n",
        "\n",
        "# Object Tracking parameters.\n",
        "# The distance an object can move along the x axis from one frame to another.\n",
        "# The paramter was decided according to an image size of 300 x 300.\n",
        "SEARCH_RANGE_X=150\n",
        "# The distance an object can move along the y axis from one frame to another.\n",
        "# The paramter was decided according to an image size of 300 x 300.\n",
        "SEARCH_RANGE_Y=20\n",
        "# No of frames you want to track.\n",
        "MEMORY=1\n",
        "\n",
        "# Dimensions for tracking images.\n",
        "HEIGHT_TRACKING = 300\n",
        "WIDTH_TRACKING = 300\n",
        "\n",
        "# Prediction confidence score.\n",
        "PREDICTION_THRESHOLD = 0.50\n",
        "area_threshold = None\n",
        "\n",
        "# Create a folder for saving prediction results.\n",
        "os.makedirs('prediction_folder', exist_ok=True)\n",
        "prediction_folder = os.path.join(os.getcwd(), 'prediction_folder')\n",
        "\n",
        "# Create a folder to troubleshoot tracking results.\n",
        "os.makedirs('tracking', exist_ok=True)\n",
        "\n",
        "# Create a folder to save detected objects from Mask RCNN\n",
        "# accpording to categories\n",
        "output_dir = \"cropped_objects\"\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3r73X-FGzz-"
      },
      "source": [
        "## Perform inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB5NOjWKLGQo"
      },
      "outputs": [],
      "source": [
        "tracking_images = {}\n",
        "features_set = []\n",
        "\n",
        "\n",
        "for frame, image_path in tqdm.tqdm(enumerate(images, start=1)):\n",
        "  # Preprocess an image.\n",
        "  original_image, resized_image = preprocess_image(image_path, HEIGHT, WIDTH)\n",
        "  input_tensor = tf.convert_to_tensor(resized_image, dtype=tf.uint8)\n",
        "\n",
        "  # Running inference with the model.\n",
        "  result = perform_detection(detection_fn, input_tensor)\n",
        "\n",
        "  if result[\"num_detections\"][0]:\n",
        "    scores = result[\"detection_scores\"][0]\n",
        "    filtered_indices = scores \u003e PREDICTION_THRESHOLD\n",
        "    result = filter_detection(result, filtered_indices)\n",
        "\n",
        "  if result[\"num_detections\"][0]:\n",
        "    # Normalize the bounding boxes according to the resized image size.\n",
        "    result[\"normalized_boxes\"] = result[\"detection_boxes\"].copy()\n",
        "    result[\"normalized_boxes\"][:, :, [0, 2]] /= HEIGHT\n",
        "    result[\"normalized_boxes\"][:, :, [1, 3]] /= WIDTH\n",
        "\n",
        "    result['detection_boxes'] = result[\"detection_boxes\"].round().astype(int)\n",
        "\n",
        "    # Adjust the image size to ensure both dimensions are at least 1024\n",
        "    # for saving images with bbx and masks.\n",
        "    height_plot, width_plot = adjust_image_size(\n",
        "        original_image.shape[0], original_image.shape[1], 1024\n",
        "    )\n",
        "    image_plot = cv2.resize(\n",
        "        original_image,\n",
        "        (width_plot, height_plot),\n",
        "        interpolation=cv2.INTER_AREA,\n",
        "    )\n",
        "    # Reframe the masks to the new size.\n",
        "    result[\"detection_masks_reframed\"] = reframe_masks(\n",
        "        result, \"normalized_boxes\", height_plot, width_plot\n",
        "    )\n",
        "\n",
        "    # Filter the prediction results based on the area threshold and\n",
        "    # remove the overlapping masks.\n",
        "    unique_indices = filter_masks(\n",
        "        result[\"detection_masks_reframed\"],\n",
        "        iou_threshold=0.08,\n",
        "        area_threshold=area_threshold,\n",
        "    )\n",
        "    result = filter_detection(result, unique_indices)\n",
        "\n",
        "  if result[\"num_detections\"][0]:\n",
        "    result['detection_classes_names'] = np.array(\n",
        "        [[str(labels[i-1]) for i in result['detection_classes'][0]]]\n",
        "    )\n",
        "\n",
        "    # Save the prediction results as an image file with bbx and masks.\n",
        "    save_bbox_masks_labels(\n",
        "        result,\n",
        "        image_plot,\n",
        "        os.path.basename(image_path),\n",
        "        prediction_folder,\n",
        "        category_index,\n",
        "        PREDICTION_THRESHOLD,\n",
        "    )\n",
        "\n",
        "    # Create object tracking data.\n",
        "    tracking_image = cv2.resize(\n",
        "        original_image,\n",
        "        (WIDTH_TRACKING, HEIGHT_TRACKING),\n",
        "        interpolation=cv2.INTER_AREA,\n",
        "    )\n",
        "    tracking_images[os.path.basename(image_path)] = tracking_image\n",
        "\n",
        "    # Reducing mask sizes in order to keep the memory required for object\n",
        "    # tracking under a threshold.\n",
        "    result[\"detection_masks_tracking\"] = np.array([\n",
        "        cv2.resize(\n",
        "            i, (WIDTH_TRACKING, HEIGHT_TRACKING), interpolation=cv2.INTER_NEAREST\n",
        "        )\n",
        "        for i in result[\"detection_masks_reframed\"]\n",
        "    ])\n",
        "\n",
        "    # In case of connected masks, keep the biggest mask and fill the holes\n",
        "    # in case of incomplete detections by Mask RCNN.\n",
        "    result[\"detection_masks_tracking\"] = np.array([\n",
        "        dilated_largest_component(i) for i in result[\"detection_masks_tracking\"]\n",
        "    ])\n",
        "\n",
        "    # Crop objects from an image using masks for color detection.\n",
        "    cropped_objects = [\n",
        "        np.where(np.expand_dims(i, -1), image_plot, 0)\n",
        "        for i in result['detection_masks_reframed']\n",
        "    ]\n",
        "\n",
        "    # Perform color detection using clustering approach.\n",
        "    dominant_colors = [\n",
        "          *map(\n",
        "              color_and_property_extractor.find_dominant_color, cropped_objects\n",
        "          )\n",
        "    ]\n",
        "    generic_color_names = color_and_property_extractor.get_generic_color_name(dominant_colors)\n",
        "\n",
        "    # Extract features.\n",
        "    features = extract_properties(\n",
        "          tracking_image, result, \"detection_masks_tracking\"\n",
        "      )\n",
        "    features[\"source_name\"] = os.path.basename(os.path.dirname(image_path))\n",
        "    features[\"image_name\"] = os.path.basename(image_path)\n",
        "    features[\"creation_time\"] = get_image_creation_time(image_path)\n",
        "    features[\"frame\"] = frame\n",
        "    features[\"detection_scores\"] = result[\"detection_scores\"][0]\n",
        "    features[\"detection_classes\"] = result[\"detection_classes\"][0]\n",
        "    features[\"detection_classes_names\"] = result[\"detection_classes_names\"][0]\n",
        "    features[\"color\"] = generic_color_names\n",
        "    features_set.append(features)\n",
        "\n",
        "\n",
        "if features_set:\n",
        "  features_df = pd.concat(features_set, ignore_index=True)\n",
        "  tracking_features = apply_tracking(\n",
        "      features_df,\n",
        "      search_range_x=SEARCH_RANGE_X,\n",
        "      search_range_y=SEARCH_RANGE_Y,\n",
        "      memory=MEMORY\n",
        "  )\n",
        "  agg_features = process_tracking_result(tracking_features)\n",
        "  counts = agg_features.groupby(\"detection_classes_names\").size()\n",
        "  counts.to_frame().to_csv(os.path.join(os.getcwd(), \"count.csv\"))\n",
        "  print(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcEjtdLL-CnZ"
      },
      "source": [
        "## Visualize Object Tracking  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yZOHwsS5GLL"
      },
      "outputs": [],
      "source": [
        "CIRCLE_RADIUS =7\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "fontScale = 1\n",
        "color = (255, 0, 0)\n",
        "groups = tracking_features.groupby('image_name')\n",
        "\n",
        "for name, group in groups:\n",
        "  img = tracking_images[name].copy()\n",
        "  for k in range(len(group)):\n",
        "    cv2.circle(img,\n",
        "               (int(group.iloc[k]['x']),int(group.iloc[k]['y'])),\n",
        "               CIRCLE_RADIUS,\n",
        "               (255,133,233),\n",
        "               -1\n",
        "    )\n",
        "    cv2.putText(img,\n",
        "                str(int(group.iloc[k]['particle'])),\n",
        "                 (int(group.iloc[k]['x']), int(group.iloc[k]['y'])),\n",
        "                font,\n",
        "                fontScale,\n",
        "                color,\n",
        "                2,\n",
        "                cv2.LINE_AA\n",
        "    )\n",
        "\n",
        "  cv2.imwrite(os.path.join('tracking',name), img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW8GKKl7-Voy"
      },
      "source": [
        "## Visualize Predictions by Categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoJ9Bd9E-ezh"
      },
      "outputs": [],
      "source": [
        "if not agg_features.empty:\n",
        "  for group_name, df in tqdm.tqdm(agg_features.groupby(\"detection_classes_names\")):\n",
        "    os.makedirs(f'{output_dir}/{group_name}', exist_ok=True)\n",
        "\n",
        "    for row in df.itertuples(index=False):\n",
        "      # Get the image\n",
        "      image = cv2.imread(os.path.join(images_dir, row.image_name))\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      new_h, new_w = image.shape[0], image.shape[1]\n",
        "\n",
        "      # Get the bounding box and resize it\n",
        "      y1, x1, y2, x2 = row.bbox_0, row.bbox_1, row.bbox_2, row.bbox_3\n",
        "      new_bbox = resize_bbox(y1, x1, y2, x2, HEIGHT_TRACKING, WIDTH_TRACKING, new_h, new_w)\n",
        "\n",
        "      # Include the score in the filename\n",
        "      score = row.detection_scores if hasattr(row, 'detection_scores') else 0.0\n",
        "      name = f'{os.path.splitext(row.image_name)[0]}_{row.particle}_{score:.2f}.png'\n",
        "\n",
        "      # Save the cropped image\n",
        "      cv2.imwrite(f'{output_dir}/{row.detection_classes_names}/{name}',\n",
        "                 image[new_bbox[0]:new_bbox[2], new_bbox[1]:new_bbox[3]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgmZvRqCDrnh"
      },
      "source": [
        "## Copying folders to my Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgmX0qaLDuj9"
      },
      "outputs": [],
      "source": [
        "destination_folder = '/mydrive/circularnet/TestModel'\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Function to safely copy directory, removing destination first if it exists\n",
        "def copytree_replace(src, dst):\n",
        "  if os.path.exists(dst):\n",
        "    shutil.rmtree(dst)\n",
        "  shutil.copytree(src, dst)\n",
        "\n",
        "# Function to safely copy file, overwriting if it exists\n",
        "def copy_replace(src, dst):\n",
        "  if os.path.exists(dst):\n",
        "    os.remove(dst)\n",
        "  shutil.copy(src, dst)\n",
        "\n",
        "copytree_replace(os.path.join(os.getcwd(), \"prediction_folder\"), os.path.join(destination_folder, \"prediction_folder\"))\n",
        "copytree_replace(os.path.join(os.getcwd(), \"cropped_objects\"),os.path.join(destination_folder, \"cropped_objects\"))\n",
        "copytree_replace(os.path.join(os.getcwd(), \"tracking\"),os.path.join(destination_folder, \"tracking\"))\n",
        "copy_replace(os.path.join(os.getcwd(), \"count.csv\"),os.path.join(destination_folder, \"count.csv\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
